{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f9afe29-bac2-4fd8-8a23-4fd1a61665ee",
      "metadata": {
        "id": "1f9afe29-bac2-4fd8-8a23-4fd1a61665ee"
      },
      "source": [
        "# My first neural network!\n",
        "## ICE 2025 - Artificial Intelligence for O&G Without the Hype\n",
        "\n",
        ">  - Complete: <a href=\"https://colab.research.google.com/github/paduapires/aapg_ice/blob/main/numpy_MLP_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        ">\n",
        ">  - Class: <a href=\"https://colab.research.google.com/github/paduapires/aapg_ice/blob/main/numpy_MLP_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "- Pedro Pesce <<pedro_pesce@petrobras.com.br>>\n",
        "- Thiago Toribio <<thiago.toribio@petrobras.com.br>>\n",
        "- Cesar Calderon <<cjcalderon@petrobras.com.br>>\n",
        "- Luiz Eduardo Queiroz <<eduardoqueiroz@petrobras.com.br>>\n",
        "- Antonio de Padua Pires <<antonio.pires@petrobras.com.br>>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0498ac4e-e860-4e71-98a4-a82a550487c7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:21:51.497441Z",
          "iopub.status.busy": "2025-09-24T18:21:51.497238Z",
          "iopub.status.idle": "2025-09-24T18:21:59.573643Z",
          "shell.execute_reply": "2025-09-24T18:21:59.572900Z",
          "shell.execute_reply.started": "2025-09-24T18:21:51.497426Z"
        },
        "id": "0498ac4e-e860-4e71-98a4-a82a550487c7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn.datasets as skdata\n",
        "import sklearn.model_selection as skms\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b10ead03-7d99-49e9-98bb-16569e0201dc",
      "metadata": {
        "id": "b10ead03-7d99-49e9-98bb-16569e0201dc"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "The DIGITS dataset is available through `sklearn.datasets`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6022451b-a538-41be-a42c-1d197cb9bd00",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:22:00.305970Z",
          "iopub.status.busy": "2025-09-24T18:22:00.305524Z",
          "iopub.status.idle": "2025-09-24T18:22:00.357666Z",
          "shell.execute_reply": "2025-09-24T18:22:00.357059Z",
          "shell.execute_reply.started": "2025-09-24T18:22:00.305944Z"
        },
        "id": "6022451b-a538-41be-a42c-1d197cb9bd00"
      },
      "outputs": [],
      "source": [
        "# Loading the data from DIGITS\n",
        "\n",
        "digits = skdata.load_digits()\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Splitting data into training and validation\n",
        "X_train, X_validation, y_train, y_validation = skms.train_test_split( ... , ... ,train_size=0.7,random_state=42)\n",
        "\n",
        "# Checking the size of the training and validation data\n",
        "print(f'{X_train.shape = }')\n",
        "print(f'{y_train.shape = }')\n",
        "print(f'{X_validation.shape = }')\n",
        "print(f'{y_validation.shape = }')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b63fda4-55fc-4bc1-8105-a3cbea09b73f",
      "metadata": {
        "id": "6b63fda4-55fc-4bc1-8105-a3cbea09b73f"
      },
      "source": [
        "### Visualizing some examples\n",
        "\n",
        "Notice how each row of the dataset is simply a list of 64 features. We can choose to interpret it as an $(8,8)$ image, but nothing in the dataset indicates the spatial relationship between neighboring pixels. More advanced neural network architectures, such as **convolutional neural networks** make use of this spatial relationship to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ec5f88-5ef6-4117-860a-7587e2dd964b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:22:00.624870Z",
          "iopub.status.busy": "2025-09-24T18:22:00.624531Z",
          "iopub.status.idle": "2025-09-24T18:22:01.645229Z",
          "shell.execute_reply": "2025-09-24T18:22:01.644119Z",
          "shell.execute_reply.started": "2025-09-24T18:22:00.624847Z"
        },
        "id": "63ec5f88-5ef6-4117-860a-7587e2dd964b"
      },
      "outputs": [],
      "source": [
        "# Plot of some examples\n",
        "\n",
        "ind = ... # YOUR CODE HERE Choose an example to visualize\n",
        "example = X[ind]\n",
        "example = example + 0*np.random.randn(*example.shape) # Add some noise?\n",
        "\n",
        "print(*example, sep=' ; ')\n",
        "fig,(ax0,ax1) = plt.subplots(1,2,figsize=(12,3))\n",
        "CS = ax0.imshow(example.reshape(1,-1),cmap='gray_r')\n",
        "ax0.set_title(f'label: {y[ind]}')\n",
        "plt.colorbar(CS,ax=ax0)\n",
        "CS = ax1.imshow(example.reshape(8,8), cmap='gray_r')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "display(example.reshape(8,8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de76f0d0-bd86-468a-9077-812770987bdc",
      "metadata": {
        "id": "de76f0d0-bd86-468a-9077-812770987bdc"
      },
      "source": [
        "### Creating the model\n",
        "\n",
        "\n",
        "While we do not know precisely how pixel intensities can be mapped to the number an image represents, there is certainly a **functional relation** between an input image and its number. We will make use of the fact that a sufficiently complex neural network is a **universal approximator** to obtain a model that uses pixel intensities (64 **features**) to estimate the probability of each example belonging to one of 10 possible classes.\n",
        "\n",
        "The simplest architecture for a neural network is the so-called **Fully-Connected Neural Network** (FCNN), sometimes called a **Multi-Layer Perceptron** (MLP). There are several possibilities for hyperparameter tuning of this type of network, including the number of hidden layers (the network's **depth**), the number of neurons per layer (its **width**), choice of **activation functions**, normalization layers, regularization, dropout, etc.\n",
        "\n",
        "In this notebook, we will use a single hidden layer with $100$ neurons and a **ReLU** activation function, followed by another linear layer and a **softmax** probability normalization, as illustrated below.\n",
        "\n",
        "<center><img src=\"https://github.com/paduapires/aapg_ice/blob/main/MLP.png?raw=1\" align=\"center\" width=\"50%\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06dd2a7-efc6-4623-b6be-79ddc006cac4",
      "metadata": {
        "id": "b06dd2a7-efc6-4623-b6be-79ddc006cac4"
      },
      "source": [
        "$$\n",
        "\\color{blue}X\\color{red}W^{(1)}\\color{black} + \\color{red}B^{(1)}\\color{black} = H^{(1)} \\quad ; \\quad ReLU(H^{(1)}) = A^{(1)} \\quad ; \\quad A^{(1)}\\color{red}W^{(2)}\\color{black}+\\color{red}B^{(2)}\\color{black} = H^{(2)} \\quad ; \\quad softmax(H^{(2)}) =\\color{purple}{P}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\color{purple}{P} = softmax\\Biggl(\\underbrace{ \\underbrace{ReLU\\bigl(\\underbrace{\\color{blue}X\\color{red}{W^{(1)}}+ \\color{red}{B^{(1)}}}_{H^{(1)}}\\bigr)}_{A^{(1)}}\n",
        "\\color{red}{W^{(2)}}+\\color{red}{B^{(2)}}}_{H^{(2)}}\\Biggr)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\color{blue}X$ is the data matrix, with $N$ rows (one for each example) and $64$ columns (one for each **feature**). It represents the model's <span style=\"color:blue\">**inputs**</span>;\n",
        "- $\\color{red}{W^{(1)}}$, $\\color{red}{B^{(1)}}$, $\\color{red}{W^{(2)}}$, $\\color{red}{B^{(2)}}$ are the adjustable <span style=\"color:red\">**parameters**</span> of the network (**weights** and **biases**), which we optimize based on some loss function in order to solve the proposed problem;\n",
        "- $H^{(1)}, A^{(1)}, H^{(2)}$ are intermediate values computed by the network. They are hidden, or **latent** values. $H^{(2)}$ in particular, being the inputs to the final softmax normalization, are also called the network's **logits**;\n",
        "- $\\displaystyle{ ReLU\\Bigl(h_{ij}^{(1)}\\Bigr) = max(0,h_{ij}^{(1)}) = \\begin{cases} h_{ij}^{(1)}&, h_{ij}^{(1)}\\geq 0 \\\\ 0&, h_{ij}^{(1)}\\le 0 \\end{cases} \\, \\, }$  is the **activation function** chosen for the hidden layer, which operates element-wise;\n",
        "- $\\displaystyle{ softmax\\Bigl(h^{(2)}_{ij}\\Bigr) = \\dfrac{e^{h^{(2)}_{ij}}}{\\displaystyle \\sum_je^{h^{(2)}_{ij}}} }$ is the final **activation function**, which normalizes each row of **logits** and outputs the probabilities matrix the $\\color{purple}{P}$, such that all output probabilities are positive and sum to $1$ for each example. This is a generalization of the sigmoid function;\n",
        "- Each element $\\color{purple}{p_{ij}}$ of the network output is the estimated probability that example $i$ belongs to class $j$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e750075-aa17-4eee-aca2-525148a05a36",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:22:02.057028Z",
          "iopub.status.busy": "2025-09-24T18:22:02.056044Z",
          "iopub.status.idle": "2025-09-24T18:22:02.063143Z",
          "shell.execute_reply": "2025-09-24T18:22:02.062280Z",
          "shell.execute_reply.started": "2025-09-24T18:22:02.057000Z"
        },
        "id": "0e750075-aa17-4eee-aca2-525148a05a36"
      },
      "outputs": [],
      "source": [
        "def softmax(X,axis=-1):\n",
        "    '''Softmax along a given axis'''\n",
        "    X_reg = X - X.max(axis=axis,keepdims=True) # Subtracting the maximum value to stabilize the exponentials\n",
        "    eX = np.exp(X_reg)\n",
        "    return eX/eX.sum(axis=axis,keepdims=True)\n",
        "\n",
        "def mlp(W1,B1,W2,B2,X,full_out=False):\n",
        "    '''MLP-type network with 1 hidden layer, ReLU activation, and softmax output:\n",
        "\n",
        "       W1,B1       ReLU       W2,B2     softmax\n",
        "    X -------> H1 ------> A1 -------> H2 -----> P\n",
        "\n",
        "     Inputs:\n",
        "    - W1, B1, W2, B2: network parameters\n",
        "    - X: input data matrix. One example per row, one feature per column.\n",
        "    - full_out: False (default) - returns only the output probability matrix.\n",
        "                True - also returns intermediate values H2, A1, H1. Useful for gradient calculation.\n",
        "\n",
        "    Outputs:\n",
        "    - P: probability matrix. One example per row, one class per column.\n",
        "    or\n",
        "    - P, H2, A1, H1: probability matrix and intermediate values.'''\n",
        "\n",
        "    H1 = X @ ... + ... # YOUR CODE HERE to apply the first linear layer on inputs X\n",
        "    A1 = np.maximum( ... , ... ) # YOUR CODE HERE for the ReLU activation function\n",
        "    H2 = ... @ ... + ... # YOUR CODE HERE to apply the second linear layer on activations A1\n",
        "    P  = ... # YOUR CODE HERE to obtain the probabilities\n",
        "    if full_out:\n",
        "        return P, H2, A1, H1\n",
        "    else:\n",
        "        return P"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575070cd-5e97-43a8-b6ed-0f881dacdc60",
      "metadata": {
        "id": "575070cd-5e97-43a8-b6ed-0f881dacdc60"
      },
      "source": [
        "## Parameter initialization\n",
        "\n",
        "Each linear layer has a weight matrix $W$ and a bias vector $B$, known as the parameters of the network. These parameters are typically initialized with small random values, according to various heuristics. We will use the default behavior of PyTorch, with weights randomly and uniformly distributed in the interval $\\left(\\dfrac{-1}{\\sqrt{N_{\\text{inputs}}}}, \\dfrac{1}{\\sqrt{N_{\\text{inputs}}}}\\right)$, where $N_{\\textrm{inputs}}$ is the number of input features *for that specific layer*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec5701b-be7d-41c6-8fc6-f496661401c3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:22:02.625828Z",
          "iopub.status.busy": "2025-09-24T18:22:02.625428Z",
          "iopub.status.idle": "2025-09-24T18:22:04.514470Z",
          "shell.execute_reply": "2025-09-24T18:22:04.513136Z",
          "shell.execute_reply.started": "2025-09-24T18:22:02.625802Z"
        },
        "id": "bec5701b-be7d-41c6-8fc6-f496661401c3"
      },
      "outputs": [],
      "source": [
        "# Initializing the network weights and biases\n",
        "np.random.seed(77)\n",
        "size_input = ...  # YOUR CODE HERE for the number of input features\n",
        "size_H = ...  # YOUR CODE HERE for the hidden layer size\n",
        "size_output = ... # YOUR CODE HERE for the number of classes\n",
        "\n",
        "# Initializing with zero-mean uniform random numbers\n",
        "W1 = (np.random.rand(size_input, size_H)-0.5) * 2/np.sqrt(size_input)  # YOUR CODE HERE for the correct size of the parameters\n",
        "B1 = (np.random.rand(            size_H)-0.5) * 2/np.sqrt(size_input)  # YOUR CODE HERE for the correct size of the parameters\n",
        "W2 = (np.random.rand(size_H,size_output)-0.5) * 2/np.sqrt(size_H)      # YOUR CODE HERE for the correct size of the parameters\n",
        "B2 = (np.random.rand(       size_output)-0.5) * 2/np.sqrt(size_H)      # YOUR CODE HERE for the correct size of the parameters\n",
        "\n",
        "fig,axs = plt.subplots(1,4,sharex='all',figsize=(12,3))\n",
        "for i,(par,par_name,ax) in enumerate(zip([W1, B1, W2, B2],['W1', 'B1', 'W2', 'B2'],axs.flat)):\n",
        "    ax.hist(par.ravel(),bins=29,color='C'+str(i),edgecolor='k')\n",
        "    ax.set_title(f'{par.size} parameters')\n",
        "    ax.set_xlabel(par_name)\n",
        "    ax.grid()\n",
        "fig.suptitle('Initial parameters')\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35a5e92-3f36-4729-b7cd-7b8fbec21b43",
      "metadata": {
        "id": "b35a5e92-3f36-4729-b7cd-7b8fbec21b43"
      },
      "source": [
        "### Objective function\n",
        "\n",
        "For classification problems, **categorical cross-entropy** is the most commonly used objective function.\n",
        "\n",
        "$$\n",
        "L = \\textrm{x-entropy}\\bigl(\\color{purple}{P},Y\\bigr) = -\\dfrac{1}{N_{\\text{examples}}} \\sum_i^{N_{\\text{examples}}} \\sum_j^{N_{\\text{classes}}} y_{ij} \\ln\\left(\\color{purple}{p_{ij}}\\right)\n",
        "$$\n",
        "\n",
        "where $y_{ij}$ is $1$ if example $i$ belongs to class $j$, and $0$ otherwise.\n",
        "\n",
        "Note that this is equivalent to taking the average of $-\\ln(\\color{purple}{p_{ij}})$ only over the probabilities estimated by the model for the example's true class. This is a generalization of the previously studied binary cross-entropy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b32832-af5f-4b61-8a8d-e9692db5d435",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:22:06.128982Z",
          "iopub.status.busy": "2025-09-24T18:22:06.128618Z",
          "iopub.status.idle": "2025-09-24T18:22:06.133960Z",
          "shell.execute_reply": "2025-09-24T18:22:06.133178Z",
          "shell.execute_reply.started": "2025-09-24T18:22:06.128958Z"
        },
        "id": "48b32832-af5f-4b61-8a8d-e9692db5d435"
      },
      "outputs": [],
      "source": [
        "# Creating the loss function (cross-entropy - classification)\n",
        "def cross_entropy(P,y):\n",
        "    '''Categorical cross-entropy\n",
        "\n",
        "    Inputs:\n",
        "     - P [N_examples,N_classes]: estimated probabilities that each example belongs to each class\n",
        "     - y [N_examples]: vector of labels'''\n",
        "    return -np.mean( np.log( np.take_along_axis( ... ,y[:,None],axis=-1) ) ) # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dbb8af2-a6fb-4c31-82fe-086661e521b1",
      "metadata": {
        "id": "9dbb8af2-a6fb-4c31-82fe-086661e521b1"
      },
      "source": [
        "### Optimization\n",
        "\n",
        "There are several possible optimization methods, but we will proceed with gradient based methods. For this, we need to compute the gradient of the objective function with respect to each network parameter, that is:\n",
        "\n",
        "$$\n",
        "\\dfrac{\\partial L}{\\partial \\color{red}{W^{(1)}}} \\, ; \\, \\dfrac{\\partial L}{\\partial \\color{red}{B^{(1)}}} \\, ; \\, \\dfrac{\\partial L}{\\partial \\color{red}{W^{(2)}}} \\, ; \\, \\dfrac{\\partial L}{\\partial \\color{red}{B^{(2)}}}\n",
        "$$\n",
        "\n",
        "These matrix derivatives are arranged in the same shape as the parameters in relation to which the derivative is calculated, that is, $\\dfrac{\\partial L}{\\partial \\color{red}{W^{(1)}}}$ is the same shape as $\\color{red}{W^{(1)}}$,  $\\dfrac{\\partial L}{\\partial \\color{red}{B^{(1)}}}$ is the same shape as $\\color{red}{B^{(1)}}$, etc.\n",
        "\n",
        "The chain rule allows us to obtain these values:\n",
        "\n",
        "\n",
        "$$\n",
        "\\color{blue}X\\color{red}W^{(1)}\\color{black} + \\color{red}B^{(1)}\\color{black} = H^{(1)} \\quad ; \\quad ReLU(H^{(1)}) = A^{(1)} \\quad ; \\quad A^{(1)}\\color{red}W^{(2)}\\color{black}+\\color{red}B^{(2)}\\color{black} = H^{(2)} \\quad ; \\quad softmax(H^{(2)}) =\\color{purple}{P}\n",
        "$$\n",
        "\n",
        "$$\n",
        "L = \\textrm{x-entropy}\\Biggl(\\underbrace{softmax\\biggl(\\underbrace{ \\underbrace{ReLU\\bigl(\\underbrace{\\color{blue}X\\color{red}{W^{(1)}}+ \\color{red}{B^{(1)}}}_{H^{(1)}}\\bigr)}_{A^{(1)}}\n",
        "\\color{red}{W^{(2)}}+\\color{red}{B^{(2)}}}_{H^{(2)}}\\biggr)}_{\\color{purple}{P}},Y\\Biggr)\n",
        "$$\n",
        "\n",
        "$$\\begin{align}\n",
        "\\frac{\\partial L}{\\partial H^{(2)}} &= \\underbrace{\\color{purple}{P}-Y}_{\\textrm{error}}\n",
        "\\\\\n",
        "\\frac{\\partial L}{\\partial \\color{red}{B^{(2)}}} &= \\frac{\\partial L}{\\partial H^{(2)}} \\underbrace{\\frac{\\partial H^{(2)}}{\\partial \\color{red}{B^{(2)}}}}_{1} & ;\\quad\n",
        "\\frac{\\partial L}{\\partial \\color{red}{W^{(2)}}} &= \\frac{\\partial L}{\\partial H^{(2)}} \\underbrace{\\frac{\\partial H^{(2)}}{\\partial \\color{red}{W^{(2)}}}}_{A^{(1)}}\n",
        "\\\\\n",
        "\\frac{\\partial L}{\\partial A^{(1)}} &= \\frac{\\partial L}{\\partial H^{(2)}} \\underbrace{\\frac{\\partial H^{(2)}}{\\partial A^{(1)}}}_{{\\color{red}{W^{(2)}}}^T}\n",
        "\\\\\n",
        "\\frac{\\partial L}{\\partial H^{(1)}} &= \\underbrace{\\frac{\\partial L}{\\partial H^{(2)}} {\\frac{\\partial H^{(2)}}{\\partial A^{(1)}}}}_{\\frac{\\partial L}{\\partial A^{(1)}}} \\,  \\underbrace{\\frac{\\partial A^{(1)}}{\\partial H^{(1)}}}_{u(H^{(1)})}  & ;\\quad u\\Bigl(h_{ij}^{(1)}\\Bigr) &= \\begin{cases} 1&, h_{ij}^{(1)}\\gt 0 \\\\ 0&, h_{ij}^{(1)}\\le 0 \\end{cases}  \n",
        "\\\\\n",
        "\\frac{\\partial L}{\\partial \\color{red}{B^{(1)}}} &=\n",
        "\\underbrace{\n",
        "    \\frac{\\partial L}{\\partial H^{(2)}} \\frac{\\partial H^{(2)}}{\\partial A^{(1)}}  \\frac{\\partial A^{(1)}}{\\partial H^{(1)}}\n",
        "           }_{\\frac{\\partial L}{\\partial H^{(1)}}} \\, \\underbrace{\\frac{\\partial H^{(1)}}{\\partial \\color{red}{B^{(1)}}}}_{1}\n",
        "& ;\\quad\n",
        "\\frac{\\partial L}{\\partial \\color{red}{W^{(1)}}} &= \\frac{\\partial L}{\\partial H^{(1)}} \\underbrace{\\frac{\\partial H^{(1)}}{\\partial \\color{red}{W^{(1)}}}}_{\\color{blue}{X}}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where for simplicity we omitted the fact that some of these products are matrix products, some are outer products and some are element-wise products. For a more step-by-step derivation of these results, see [https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf].\n",
        "\n",
        "It's important to note that the efficient operationalization of the chain rule in neural networks to calculate the gradients, known as **backpropagation** (term coined in the 80's by Rumelhart et al, inspired by a work of Rosenblatt), was essential for the training of larger scale neural networks to be feasible. While we implement the gradient calculation here manually in a rather naive way for didactic purposes, neural network frameworks such as Pytorch and Tensorflow go to great lengths to optimize this calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc445988-6958-48fd-a51b-a5fb707de25e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:22:06.751865Z",
          "iopub.status.busy": "2025-09-24T18:22:06.751475Z",
          "iopub.status.idle": "2025-09-24T18:22:06.759668Z",
          "shell.execute_reply": "2025-09-24T18:22:06.758616Z",
          "shell.execute_reply.started": "2025-09-24T18:22:06.751814Z"
        },
        "id": "cc445988-6958-48fd-a51b-a5fb707de25e"
      },
      "outputs": [],
      "source": [
        "def grad(X,y,P,H2,A1,H1,W1,B1,W2,B2):\n",
        "    '''Gradient of L for the neural network.\n",
        "    Receives everything as input, to avoid having to perform the forward pass again.\n",
        "\n",
        "    Expected dimensions for each input:\n",
        "\n",
        "    X  [n_batch, n_in] : Input matrix\n",
        "    y  [n_batch]       : label vector\n",
        "    P  [n_batch, n_out]: Matrix with estimated probabilities\n",
        "    H1 [n_batch, n_H  ]  ;  W1 [n_in,  n_H ] ; B1 [n_H] : first linear layer\n",
        "    A1 [n_batch, n_H  ]  : activations\n",
        "    H2 [n_batch, n_out]  ;  W2 [n_H , n_out] ; B2 [n_out] : second linear layer\n",
        "\n",
        "       W1,B1       ReLU       W2,B2      softmax      cross-entropy\n",
        "    X -------> H1 ------> A1 -------> H2 ------> y_pred --------> L\n",
        "\n",
        "    Outputs:\n",
        "     - dLdW1  [n_in,  n_H ]\n",
        "     - dLdB1  [n_H]\n",
        "     - dLdW2  [n_H , n_out]\n",
        "     - dLdB2  [n_out]\n",
        "    '''\n",
        "\n",
        "    n_batch,n_out = P.shape\n",
        "    n = np.arange(0,n_batch*n_out,n_out) # for linear indexing\n",
        "    Err = P.copy() # [n_batch,n_out] # initializing the error matrix\n",
        "    Err.flat[n+y] -= 1  # subtracting 1 from the true classes\n",
        "\n",
        "    dLdH2  = Err   # [n_batch, n_out]\n",
        "    dH2dB2 = 1     # [n_out]\n",
        "    dH2dW2 = A1    # [n_batch, n_H]\n",
        "    dH2dA1 = W2.T  # [n_out  , n_H]\n",
        "    dA1dH1 = H1>0  # [n_batch, n_H]\n",
        "    dH1dB1 = 1     # [n_H]\n",
        "    dH1dW1 = X     # [n_batch, n_in]\n",
        "    dH1dX  = W1.T  # [n_H    , n_in]  (unnecessary)\n",
        "\n",
        "    dLdB2 = dLdH2 * dH2dB2  # [n_batch, n_out] * [n_out] -> [n_batch, n_out]\n",
        "    dLdW2 = dLdH2[:,None,:]*dH2dW2[:,:,None] # [n_batch, 1, n_out] * [n_batch, n_H, 1] -> [n_batch, n_H, n_out]\n",
        "\n",
        "    dLdA1  = dLdH2 @ dH2dA1  # [n_batch, n_out] @ [n_out, n_H] -> [n_batch, n_H]\n",
        "    dLdH1  = dLdA1 * dA1dH1  # [n_batch, n_H] * [n_batch, n_H] -> [n_batch, n_H ]\n",
        "\n",
        "    dLdB1 = dLdH1 * dH1dB1 # [n_batch, n_H] * [n_H] -> [n_batch, n_H]\n",
        "    dLdW1 = dLdH1[:,None,:] * dH1dW1[:,:,None] # [n_batch, 1, n_H] * [n_batch, n_in, 1] -> [n_batch, n_in, n_H]\n",
        "\n",
        "    return dLdW1.mean(axis=0), dLdB1.mean(axis=0), dLdW2.mean(axis=0), dLdB2.mean(axis=0) # taking the average over the batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17926673-3d0c-4d4e-8ee5-c8d47f2f33b0",
      "metadata": {
        "id": "17926673-3d0c-4d4e-8ee5-c8d47f2f33b0"
      },
      "source": [
        "### Learning Loop (Optimization)\n",
        "\n",
        "The process of searching for **optimum values** for the **weights** and **biases** occurs in the learning loop. This consists of some basic steps:\n",
        "\n",
        "> 1) **Forward** stage:\n",
        "> > - Computation of the estimated **outputs** for a set of inputs\n",
        "> > - Computation of the **loss** function\n",
        "> 2) **Backward** stage:\n",
        "> > - Computation of the **gradient** of the loss function with respect to each weight and bias\n",
        "> 3) **Learning** stage:\n",
        "> > - **Update** of weights and biases\n",
        "\n",
        "In this example, we will use gradient descent with a learning rate `lr=0.04`.\n",
        "\n",
        "Parameter updates can be done with different frequencies:\n",
        "- Once for every example: **stochastic gradient descent** (SGD), or **online** gradient descent;\n",
        "- Once every `n_batch` examples: **mini-batch gradient descent**, frequently (but incorrectly, including by Pytorch) called **Stochastic Gradient Descent** (SGD);\n",
        "- Once per epoch:  (classical, **batch** or **full-batch**) gradient descent.\n",
        "\n",
        "While **full-batch** gradient descent performs only one (computationally expensive) update per **epoch** (one complete pass through all training examples), stochastic and mini-batch gradient descent allow for several (computationally cheaper, but noisy) parameter updates per **epoch**, usually achieving faster training times."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing empty lists\n",
        "loss_train_vanilla = []\n",
        "loss_train_augmented = []\n",
        "loss_validation  = []\n",
        "\n",
        "epoch = 0"
      ],
      "metadata": {
        "id": "cB_KEQnbU5zj"
      },
      "id": "cB_KEQnbU5zj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46645969-dc14-43c1-82cf-cece6aceb89d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:26:40.870089Z",
          "iopub.status.busy": "2025-09-24T18:26:40.869417Z",
          "iopub.status.idle": "2025-09-24T18:26:57.380743Z",
          "shell.execute_reply": "2025-09-24T18:26:57.379649Z",
          "shell.execute_reply.started": "2025-09-24T18:26:40.870061Z"
        },
        "id": "46645969-dc14-43c1-82cf-cece6aceb89d"
      },
      "outputs": [],
      "source": [
        "data_augmentation = ... # add noise to training examples?\n",
        "batches = ... # full-batch (1) or mini-batch (6, for example)\n",
        "\n",
        "start = time.time()\n",
        "max_epochs = 150\n",
        "lr = 0.04\n",
        "n_batch = int(np.ceil(X_train.shape[0]/batches))\n",
        "for i_epoch in range(1,max_epochs+1):\n",
        "    l_vanilla = 0 # initializing for the epoch\n",
        "    l_augmented = 0\n",
        "    for ind in range(0,X_train.shape[0],n_batch): # mini batches\n",
        "        X_batch = X_train[ind:ind+n_batch]\n",
        "        y_batch = y_train[ind:ind+n_batch]\n",
        "\n",
        "        # 1) Forward pass\n",
        "        y_pred,H2,A1,H1 = mlp(W1,B1,W2,B2,X_batch,full_out=True) # computes y_predicted (and likely errs)\n",
        "        X_grad = X_batch # X to use in the gradient calculation\n",
        "        l_vanilla += cross_entropy(y_pred, y_batch) # compares the prediction with the true label\n",
        "\n",
        "        if data_augmentation:\n",
        "            X_augmented = X_batch + 3.0*np.random.randn(*X_batch.shape) # Data augmentation?\n",
        "            y_pred,H2,A1,H1 = mlp(W1,B1,W2,B2,X_augmented,full_out=True) # adding noise (data augmentation)\n",
        "            X_grad = X_augmented #  X to use in the gradient calculation\n",
        "            l_augmented += cross_entropy(y_pred, y_batch) # compares the prediction with the true label\n",
        "\n",
        "        # 2) Backward pass: calculating the gradients\n",
        "        dW1, dB1, dW2, dB2 = grad(X_grad,y_batch,y_pred,H2,A1,H1,W1,B1,W2,B2)\n",
        "\n",
        "        # 3) Updating the parameters\n",
        "        W1 -=  ...  # YOUR CODE HERE\n",
        "        B1 -=  ...  # YOUR CODE HERE\n",
        "        W2 -=  ...  # YOUR CODE HERE\n",
        "        B2 -=  ...  # YOUR CODE HERE\n",
        "\n",
        "    loss_train_vanilla.append(l_vanilla/batches)\n",
        "    loss_train_augmented.append(l_augmented/batches) # Saving for posterity\n",
        "\n",
        "    # Checking the performance on the validation set\n",
        "    y_pred = mlp(W1,B1,W2,B2,X_validation)\n",
        "    l_validation = cross_entropy(y_pred, y_validation)\n",
        "    loss_validation.append(l_validation)\n",
        "    if ((epoch+i_epoch)%50) == 0 or i_epoch in [0,max_epochs]:\n",
        "        end = time.time()\n",
        "        t = end-start\n",
        "        print(f'Epoch {epoch+i_epoch:4d}: Loss Train augmented {loss_train_augmented[-1]:.5f}; Loss Train vanilla {loss_train_vanilla[-1]:.5f}; Loss Validation {loss_validation[-1]:.5f}; Time {t:.5f}s')\n",
        "        start = time.time()\n",
        "epoch += max_epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39c1f35c-e0ff-49cf-8006-308ddb26d59e",
      "metadata": {
        "id": "39c1f35c-e0ff-49cf-8006-308ddb26d59e"
      },
      "source": [
        "### Visualizing the training progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e52d99-2737-4420-948a-cf27a9e8d159",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:27:00.167734Z",
          "iopub.status.busy": "2025-09-24T18:27:00.167413Z",
          "iopub.status.idle": "2025-09-24T18:27:02.719695Z",
          "shell.execute_reply": "2025-09-24T18:27:02.718587Z",
          "shell.execute_reply.started": "2025-09-24T18:27:00.167713Z"
        },
        "id": "e3e52d99-2737-4420-948a-cf27a9e8d159"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.semilogy(loss_train_augmented, label='Training augmented')\n",
        "plt.plot(loss_validation, label='Validation')\n",
        "plt.plot(loss_train_vanilla, label='Training vanilla')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch'); plt.ylabel('')\n",
        "plt.grid(which='both')\n",
        "plt.show()\n",
        "\n",
        "fig,axs = plt.subplots(1,4,sharex='all',figsize=(12,3))\n",
        "for i,(par,par_name,ax) in enumerate(zip([W1, B1, W2, B2],['W1', 'B1', 'W2', 'B2'],axs.flat)):\n",
        "    ax.hist(par.ravel(),bins=29,color='C'+str(i),edgecolor='k')\n",
        "    ax.set_title(f'{par.size} parameters')\n",
        "    ax.set_xlabel(par_name)\n",
        "    ax.grid()\n",
        "fig.suptitle('Final parameters')\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation\n",
        "\n",
        "ML models tend to **generalize** better when trained on more diverse examples. However, it is not always feasible to get more training data, either because data is not available, or, more commonly, available data is not labeled. Strategies which we can use to diversify the used training data include generating **synthetic data** from scratch and modifying existing training data, called **data augmentation**.\n",
        "\n",
        "Common augmentation techniques include **noise injection**, **geometric transformations** (mirroring, rotating, deforming images, etc), **frequency filtering**, **color transformations**, among others. The choice of data augmentation strategies is heavily domain specific, and must be carefully guided by the **domain expert** to ensure training data diversity while still closely matching real world data.\n",
        "\n",
        "As an example, mirroring, rotating and flipping medical microscopy images upside down are all valid data augmentation strategies. However, for seismic images, only mirroring and small rotations would still look like real world data."
      ],
      "metadata": {
        "id": "zTrBR-tbZoF2"
      },
      "id": "zTrBR-tbZoF2"
    },
    {
      "cell_type": "markdown",
      "id": "529e7c3e-cc83-411c-94a3-4e5696ed6134",
      "metadata": {
        "id": "529e7c3e-cc83-411c-94a3-4e5696ed6134"
      },
      "source": [
        "### Model performance on the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c74f768-e2b8-4108-8781-746209afe26f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:27:26.639120Z",
          "iopub.status.busy": "2025-09-24T18:27:26.638773Z",
          "iopub.status.idle": "2025-09-24T18:27:27.396929Z",
          "shell.execute_reply": "2025-09-24T18:27:27.391605Z",
          "shell.execute_reply.started": "2025-09-24T18:27:26.639098Z"
        },
        "id": "0c74f768-e2b8-4108-8781-746209afe26f"
      },
      "outputs": [],
      "source": [
        "py_pred = mlp(W1,B1,W2,B2,X_train) # estimated class probabilities for the TRAINING set\n",
        "\n",
        "y_pred = np. ... (py_pred, axis=1) # most likely class for each example\n",
        "labels = np.arange(py_pred.shape[-1]) # class sequence\n",
        "\n",
        "cm = skm.confusion_matrix(y_train, y_pred)\n",
        "sns.heatmap(cm, annot=cm, cmap='Blues', fmt='d', xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Rows: label\\nColumns: prediction')\n",
        "plt.show()\n",
        "\n",
        "crep = skm.classification_report(y_train, y_pred, zero_division=0)\n",
        "print(crep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09acc102-a7b2-477c-9a7e-0563ae777fe3",
      "metadata": {
        "id": "09acc102-a7b2-477c-9a7e-0563ae777fe3"
      },
      "source": [
        "### Model evaluation on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db8d8825-3f35-46f6-acdf-aaae2a4bd1ad",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:27:50.097805Z",
          "iopub.status.busy": "2025-09-24T18:27:50.097527Z",
          "iopub.status.idle": "2025-09-24T18:27:50.830553Z",
          "shell.execute_reply": "2025-09-24T18:27:50.828795Z",
          "shell.execute_reply.started": "2025-09-24T18:27:50.097786Z"
        },
        "id": "db8d8825-3f35-46f6-acdf-aaae2a4bd1ad"
      },
      "outputs": [],
      "source": [
        "py_pred = mlp(W1,B1,W2,B2,X_validation) # YOUR CODE HERE for estimated probabilities on the VALIDATION set\n",
        "\n",
        "y_pred = np. ... (py_pred, axis=1) # YOUR CODE HERE for the most likely class for each example\n",
        "\n",
        "cm = skm.confusion_matrix(y_validation, y_pred)\n",
        "sns.heatmap(cm, annot=cm, cmap='Reds', fmt='d', xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Rows: label\\nColumns: prediction')\n",
        "plt.show()\n",
        "\n",
        "crep = skm.classification_report(y_validation, y_pred, zero_division=0)\n",
        "print(crep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3dce58a-132a-4ada-ad59-f6a79dddc522",
      "metadata": {
        "id": "d3dce58a-132a-4ada-ad59-f6a79dddc522"
      },
      "source": [
        "### Visualization of incorrect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "421093f8-da8a-4031-af96-91ed2a3b154d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:28:40.127670Z",
          "iopub.status.busy": "2025-09-24T18:28:40.127455Z",
          "iopub.status.idle": "2025-09-24T18:28:42.332271Z",
          "shell.execute_reply": "2025-09-24T18:28:42.330559Z",
          "shell.execute_reply.started": "2025-09-24T18:28:40.127656Z"
        },
        "id": "421093f8-da8a-4031-af96-91ed2a3b154d"
      },
      "outputs": [],
      "source": [
        "y_err  = y_pred[y_pred != y_validation] # incorrect predictions\n",
        "yr_err = y_validation[y_pred != y_validation] # true values of incorrect predictions\n",
        "X_err  = X_validation[y_pred != y_validation] # incorrect predictions image\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14,4))\n",
        "n_wrong = X_err.shape[0]\n",
        "nrow = 2\n",
        "ncol = int(np.ceil(n_wrong/nrow))\n",
        "for ind in range(n_wrong):\n",
        "    plt.subplot(nrow,ncol,ind+1)\n",
        "    plt.imshow(X_err[ind].reshape(8,8),cmap='gray_r')\n",
        "    plt.title(f'Predicted {y_err[ind]}, Real {yr_err[ind]}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "595b228b-4081-45c4-bea5-74ed56b4f460",
      "metadata": {
        "id": "595b228b-4081-45c4-bea5-74ed56b4f460"
      },
      "source": [
        "### Drawing a new number for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9674f93f-897b-4d2d-894b-f096c20843c3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T18:30:28.618797Z",
          "iopub.status.busy": "2025-09-24T18:30:28.618299Z",
          "iopub.status.idle": "2025-09-24T18:30:28.894149Z",
          "shell.execute_reply": "2025-09-24T18:30:28.892695Z",
          "shell.execute_reply.started": "2025-09-24T18:30:28.618768Z"
        },
        "id": "9674f93f-897b-4d2d-894b-f096c20843c3"
      },
      "outputs": [],
      "source": [
        "# Draw a digit here, with intensities between 0 and 15\n",
        "my_num = np.array([ 00., 00., 00., 00., 00., 00., 00.,  00.,\n",
        "                    00., 00., 00., 00., 00., 00., 00.,  00.,\n",
        "                    00., 00., 00., 00., 00., 00., 00.,  00.,\n",
        "                    00., 00., 00., 00., 00., 00., 00.,  00.,\n",
        "                    00., 00., 00., 00., 00., 00., 00.,  00.,\n",
        "                    00., 00., 00., 00., 00., 00., 00.,  00.,\n",
        "                    00., 00., 00., 00., 00., 00., 00.,  00.,\n",
        "                    00., 00., 00., 00., 00., 00., 00.,  00.,]).astype(float) # YOUR CODE HERE to draw a number\n",
        "my_pred = mlp(W1,B1,W2,B2,my_num)\n",
        "for i,p in enumerate(my_pred):\n",
        "    print(f'{i}:{p:.3f}',sep='',end='  ;  ') # printing the estimated probabilities\n",
        "plt.imshow(my_num.reshape(8,8),cmap='gray_r')\n",
        "plt.title(f'The image is a {np.argmax(my_pred)} with {my_pred.max():.1%} probability')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94ec408-3040-4579-aec0-b5e9f413d88b",
      "metadata": {
        "id": "c94ec408-3040-4579-aec0-b5e9f413d88b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}