{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# K-means\n",
        "## ICE 2025 - Artificial Intelligence for O&G Without the Hype\n",
        "\n",
        ">  - Complete: <a href=\"https://colab.research.google.com/github/paduapires/aapg_ice/blob/main/Kmeans_thin_section_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        ">\n",
        ">  - Class: <a href=\"https://colab.research.google.com/github/paduapires/aapg_ice/blob/main/Kmeans_thin_section_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "- Pedro Pesce <<pedro_pesce@petrobras.com.br>>\n",
        "- Thiago Toribio <<thiago.toribio@petrobras.com.br>>\n",
        "- Cesar Calderon <<cjcalderon@petrobras.com.br>>\n",
        "- Luiz Eduardo Queiroz <<eduardoqueiroz@petrobras.com.br>>\n",
        "- Antonio de Padua Pires <<antonio.pires@petrobras.com.br>>"
      ],
      "metadata": {
        "id": "FMf2BOreXJqF"
      },
      "id": "FMf2BOreXJqF"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.cluster as skc\n",
        "import matplotlib.colors as clrs\n",
        "import PIL\n",
        "import urllib"
      ],
      "metadata": {
        "id": "lrYBLKRQW-UB"
      },
      "id": "lrYBLKRQW-UB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b957556f-797e-4815-90e3-dccbef7eeb20",
      "metadata": {
        "id": "b957556f-797e-4815-90e3-dccbef7eeb20"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "Clustering is a fundamental **unsupervised** machine learning task that aims to discover hidden patterns in data. The core principle of clustering revolves around two key concepts: **intra-cluster similarity** (objects within the same cluster should be similar) and **inter-cluster dissimilarity** (objects from different clusters should be different). This balance creates meaningful groupings that can reveal insights about the data structure.\n",
        "\n",
        "Let's examine 3 thin section images, called `section1.jpg`, `section2.jpg` and  `section3.jpg`, available in the mains folder of the repository `https://github.com/paduapires/aapg_ice/raw/main/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae0a3b73-0103-4e7c-ab6f-084f2d485c66",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-18T18:35:17.390359Z",
          "iopub.status.busy": "2025-09-18T18:35:17.389694Z",
          "iopub.status.idle": "2025-09-18T18:35:18.807223Z",
          "shell.execute_reply": "2025-09-18T18:35:18.806106Z",
          "shell.execute_reply.started": "2025-09-18T18:35:17.390315Z"
        },
        "id": "ae0a3b73-0103-4e7c-ab6f-084f2d485c66"
      },
      "outputs": [],
      "source": [
        "url = 'https://github.com/paduapires/aapg_ice/raw/main/'\n",
        "files = ['section1.jpg', ... , ... ] # YOUR CODE HERE\n",
        "imgs = [np.array(PIL.Image.open(urllib.request.urlopen(url+file))) for file in files]\n",
        "n_imgs = len(imgs)\n",
        "fig,axs = plt.subplots(1,n_imgs,figsize=(12,3))\n",
        "for ax,img in zip(axs,imgs):\n",
        "    ax.imshow(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcf5286-2453-4df1-8923-247cf9698105",
      "metadata": {
        "id": "cbcf5286-2453-4df1-8923-247cf9698105"
      },
      "source": [
        "## Feature space\n",
        "\n",
        "The $R$, $G$ and $B$ values of each pixel can be considered features in a 3D space. Pixels with similar characteristics (*features*, colors) will be close by, while those with significantly different colors will be far apart.\n",
        "\n",
        "It is our hope that points will *cluster* around different regions in feature space, and identifying these *clusters* can give us insight into the structure of the dataset.\n",
        "\n",
        "In order to use the usual ML functions, we must organize our dataset such that the $X$ matrix contains one example (pixel) per row, one feature (R,G,B) per column:\n",
        "\n",
        "$$X = \\begin{bmatrix}\n",
        "R_{1}  & G_{1}   & B_{1} \\\\\n",
        "R_{2}  & G_{2}   & B_{2} \\\\\n",
        "\\vdots & \\vdots  & \\vdots \\\\\n",
        "R_{N}  & G_{N}   & B_{N} \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a67287-31f7-4d8e-ba5e-3ee1cc90e2e1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-18T18:35:18.810264Z",
          "iopub.status.busy": "2025-09-18T18:35:18.809521Z",
          "iopub.status.idle": "2025-09-18T18:35:19.710440Z",
          "shell.execute_reply": "2025-09-18T18:35:19.705793Z",
          "shell.execute_reply.started": "2025-09-18T18:35:18.810236Z"
        },
        "id": "29a67287-31f7-4d8e-ba5e-3ee1cc90e2e1"
      },
      "outputs": [],
      "source": [
        "X = np.vstack([img.reshape( ... , ... ) for img in imgs])/255  # YOUR CODE HERE. Feature matrix. One example per line, one feature per column\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "sub = 150 # subsampling, for speed\n",
        "ax.scatter(*X[::sub,:].T,c=X[::sub,:])\n",
        "ax.set_xlabel('R'); ax.set_ylabel('G'); ax.set_zlabel('B')\n",
        "ax.set_title('RGB feature space')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32a3cf2c-e314-489f-bb9e-773087224f7b",
      "metadata": {
        "id": "32a3cf2c-e314-489f-bb9e-773087224f7b"
      },
      "source": [
        "## K-Means Algorithm\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "K-Means, developed by Stuart Lloyd in 1957, belongs to the clustering family and aims to partition a dataset into **$K$ clusters** where $K$ is a hyperparameter (that is, a parameter that is not optimized by the K-Means algorithm). The algorithm's name reflects its core strategy: finding $K$ centroids (centers) and assigning each data point to the nearest centroid, with each cluster represented by the **mean** of its constituent points.\n",
        "\n",
        "The algorithm seeks to minimize the **Within-Cluster Sum of Squares (WCSS)**, also known as **inertia**:\n",
        "\n",
        "$$\n",
        "WCSS = \\sum_{j=1}^K \\sum_{i \\in C_j} \\left\\lVert\\vec{x}_i-\\vec{\\mu}_j \\right\\rVert^2\n",
        "$$\n",
        "\n",
        "where $j$ is an index that runs over each of the $K$ clusters, the $i$ sum runs only over the points assigned to the cluster $C_j$ and $\\vec{\\mu}_j$ is the *centroid* of the cluster:\n",
        "\n",
        "$$\n",
        "\\vec{\\mu}_j = \\frac{1}{\\left\\lvert C_j \\right\\rvert} \\sum_{i \\in C_j} \\vec{x}_i\n",
        "$$\n",
        "\n",
        "where $\\left\\lvert C_j \\right\\rvert$ is the number of points in set $C_j$, its *cardinality*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a80b42df-b1b7-45e1-9a77-746a2d08d62f",
      "metadata": {
        "id": "a80b42df-b1b7-45e1-9a77-746a2d08d62f"
      },
      "source": [
        "### Algorithm Steps\n",
        "\n",
        "The naive K-Means algorithm (also called Lloyd's algorithm) follows an iterative process with four main steps in order to obtain its partitioning:\n",
        "\n",
        "1. **Initialization**: involves choosing $K$ points as initial centroids. While random selection is simple, more sophisticated methods like K-Means++ can lead to better results by choosing centroids that are far from each other.\n",
        "\n",
        "2. **Assignment Step**: assigns each data point to the cluster whose centroid is closest:\n",
        "$C_j = \\{ \\vec{x}: \\left\\lVert \\vec{x} - \\vec{\\mu}_j \\right\\rVert^2 \\leq \\left\\lVert \\vec{x} - \\vec{\\mu}_i \\right\\rVert^2 \\quad \\forall i, 1 \\leq i \\leq K \\}$\n",
        "\n",
        "3. **Update Step**: recalculates centroids as the mean of points assigned to each cluster:\n",
        "$\\displaystyle \\vec{\\mu}_j = \\frac{1}{\\left\\lvert C_j \\right\\rvert} \\sum_{i \\in C_j} \\vec{x}_i$\n",
        "\n",
        "4. **Convergence Check**: determines whether to continue iterating. The algorithm stops when centroids don't move significantly, the objective function doesn't improve meaningfully, or a maximum number of iterations is reached. Otherwise, return to the **assignment step**.\n",
        "\n",
        "**Important:** since K-means uses **distance** in **feature space** to make cluster assignements, it is usually worth it to ensure that all features have **similar scales** (or that their scales correspond to their relative importance for determining the assigned cluster). Since the RGB features we are using are similar in scale, there is no need to preprocess the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85b89305-90f4-40be-ae16-8a69a3314d27",
      "metadata": {
        "id": "85b89305-90f4-40be-ae16-8a69a3314d27"
      },
      "source": [
        "### Scikit-Learn implementation\n",
        "\n",
        "A user friendly implementation is available via `skc.KMeans`, which implements the `.fit` and `.predict` methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac004e08-f2ce-40f2-80ed-6d3288420d3f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-18T18:35:19.713732Z",
          "iopub.status.busy": "2025-09-18T18:35:19.712127Z",
          "iopub.status.idle": "2025-09-18T18:35:20.595189Z",
          "shell.execute_reply": "2025-09-18T18:35:20.594211Z",
          "shell.execute_reply.started": "2025-09-18T18:35:19.713698Z"
        },
        "id": "ac004e08-f2ce-40f2-80ed-6d3288420d3f"
      },
      "outputs": [],
      "source": [
        "n_clusters = 4 # K, the number of clusters\n",
        "KM = skc.KMeans(n_clusters=n_clusters,random_state=1) # instantiating the clustering object\n",
        "display(KM)\n",
        "\n",
        "KM. ... (X) # YOUR CODE HERE. training\n",
        "\n",
        "display(KM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a54de3-7dea-4d52-9f6c-c584ffee8150",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-18T18:35:20.597388Z",
          "iopub.status.busy": "2025-09-18T18:35:20.596616Z",
          "iopub.status.idle": "2025-09-18T18:35:22.538993Z",
          "shell.execute_reply": "2025-09-18T18:35:22.537884Z",
          "shell.execute_reply.started": "2025-09-18T18:35:20.597344Z"
        },
        "id": "a3a54de3-7dea-4d52-9f6c-c584ffee8150"
      },
      "outputs": [],
      "source": [
        "y = KM. ... ( ... ) # YOUR CODE HERE. the cluster assignment of each data point\n",
        "\n",
        "# Creating a colormap based on the color of each centroid\n",
        "custom_cmap = clrs.LinearSegmentedColormap.from_list(\"custom_colormap\", KM.cluster_centers_,n_clusters)\n",
        "\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "axs = [fig.add_subplot(1,2,i+1,projection='3d') for i in range(2)]\n",
        "sub = 250 # subsampling, for speed\n",
        "\n",
        "# Scatter plot with \"random\" colors for each cluster\n",
        "CS = axs[0].scatter(*X[::sub,:].T,c=y[::sub],cmap='tab10',vmin=-0.5,vmax=9.5)\n",
        "cbar = plt.colorbar(CS,ax=axs[0])\n",
        "cbar.set_ticks(range(n_clusters))\n",
        "\n",
        "# Scatter plot with \"centroid\" colormap\n",
        "CS = axs[1].scatter(*X[::sub,:].T,c=y[::sub],cmap=custom_cmap,vmin=-0.5,vmax=n_clusters-0.5)\n",
        "cbar = plt.colorbar(CS,ax=axs[1])\n",
        "cbar.set_ticks(range(n_clusters))\n",
        "\n",
        "for ax in axs:\n",
        "    ax.set_xlabel('R'); ax.set_ylabel('G'); ax.set_zlabel('B')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9555f94-522d-4f24-9776-acc503764517",
      "metadata": {
        "id": "b9555f94-522d-4f24-9776-acc503764517"
      },
      "source": [
        "### Theoretical Properties\n",
        "\n",
        "K-Means has several important theoretical guarantees. The algorithm **always converges** to a local minimum because the objective function decreases monotonically at each iteration. Convergence usually occurs within a few iterations, making the algorithm computationally efficient for typical datasets.\n",
        "\n",
        "The **computational complexity** is $O(n \\times k \\times i \\times d)$, where $n$ is the number of data points, $k$ is the number of clusters, $i$ is the number of iterations, and $d$ is the number of dimensions. This linear complexity in the number of points makes K-Means scalable to large datasets.\n",
        "\n",
        "### Strengths and Limitations\n",
        "\n",
        "K-Means offers several advantages that explain its popularity. The algorithm is **simple and intuitive**, making it easy to understand and implement. It's **computationally efficient** and scales well to large datasets.\n",
        "\n",
        "However, K-Means has significant limitations. It requires **specifying $K$ beforehand**, which can be challenging without domain knowledge. The algorithm is **sensitive to initialization**, potentially converging to suboptimal local minima with different starting points. K-Means **assumes spherical clusters** of similar sizes, making it unsuitable for elongated or irregularly shaped clusters. The algorithm is also **sensitive to outliers** since a single extreme point can significantly shift a centroid, and it struggles with **clusters of very different densities or sizes**.\n",
        "\n",
        "While the the algorithm produces cluster assignments for each point, it does **not** provide the physical (geological) significance of each cluster. It is up to the **domain expert** to interpret the meaning of each cluster and evaluate if the results are meaningful and useful to the problem at hand.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55723e9-b4c4-438c-9573-13470863457e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-18T18:38:55.352499Z",
          "iopub.status.busy": "2025-09-18T18:38:55.352153Z",
          "iopub.status.idle": "2025-09-18T18:38:59.783378Z",
          "shell.execute_reply": "2025-09-18T18:38:59.781626Z",
          "shell.execute_reply.started": "2025-09-18T18:38:55.352476Z"
        },
        "id": "f55723e9-b4c4-438c-9573-13470863457e"
      },
      "outputs": [],
      "source": [
        "imgs_y = []\n",
        "i = 0\n",
        "for img in imgs: # unpacking assignments back into the original img shapes\n",
        "    shape = img.shape\n",
        "    npoints = shape[0]*shape[1]\n",
        "    imgs_y.append(y[i:i+npoints].reshape(shape[:-1]))\n",
        "    i += npoints\n",
        "fig,axs = plt.subplots(3,n_imgs,figsize=(14,10), constrained_layout=True)\n",
        "for i,(img,img_y) in enumerate(zip(imgs,imgs_y)):\n",
        "    CS = axs[0,i].imshow(img_y,cmap='tab10',vmin=-0.5,vmax=9.5)\n",
        "    cbar = plt.colorbar(CS,ax=axs[0,i], shrink=0.8)\n",
        "    cbar.set_ticks(range(n_clusters))\n",
        "    axs[1,i].imshow(img)\n",
        "    CS = axs[2,i].imshow(img_y,cmap=custom_cmap,vmin=-0.5,vmax=n_clusters-0.5)\n",
        "    cbar = plt.colorbar(CS,ax=axs[2,i], shrink=0.8)\n",
        "    cbar.set_ticks(range(n_clusters))\n",
        "axs[0,0].set_ylabel('Clusters (random colors)')\n",
        "axs[1,0].set_ylabel('Original images')\n",
        "axs[2,0].set_ylabel('Clusters (centroid colors)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application example: porosity estimation\n",
        "\n",
        "In our examples, we see that cluster $0$ is mostly composed of **pore** pixels. After **interpreting** the geological significance of this clusters, we can use the cluster assignments to estimate **porosity**.\n",
        "\n",
        "- Could the obtained cluster assignments be used to estimate clay fraction?"
      ],
      "metadata": {
        "id": "n0OMDiyI-BgK"
      },
      "id": "n0OMDiyI-BgK"
    },
    {
      "cell_type": "code",
      "source": [
        "pore_cluster = ...\n",
        "\n",
        "print('Porosity estimation:')\n",
        "for i,img_y in enumerate(imgs_y):\n",
        "    porosity = ... # YOUR CODE HERE to obtain the fraction of \"pore\" pixels in img_y\n",
        "    print(f'Image {i+1}: {porosity:.1%}')\n"
      ],
      "metadata": {
        "id": "7drGDbNkYJdd"
      },
      "id": "7drGDbNkYJdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WN8HfLUsABYb"
      },
      "id": "WN8HfLUsABYb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}