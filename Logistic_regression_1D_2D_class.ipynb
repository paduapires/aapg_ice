{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e8f5a70-8f41-4bd5-8fdc-3ed9e29871c1",
      "metadata": {
        "id": "6e8f5a70-8f41-4bd5-8fdc-3ed9e29871c1"
      },
      "source": [
        "# Logistic Regression\n",
        "## ICE 2025 - Artificial Intelligence for O&G Without the Hype\n",
        "\n",
        "\n",
        ">  - Complete: <a href=\"https://colab.research.google.com/github/paduapires/aapg_ice/blob/main/Logistic_regression_1D_2D_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        ">\n",
        ">  - Class: <a href=\"https://colab.research.google.com/github/paduapires/aapg_ice/blob/main/Logistic_regression_1D_2D_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "- Pedro Pesce <<pedro_pesce@petrobras.com.br>>\n",
        "- Thiago Toribio <<thiago.toribio@petrobras.com.br>>\n",
        "- Cesar Calderon <<cjcalderon@petrobras.com.br>>\n",
        "- Luiz Eduardo Queiroz <<eduardoqueiroz@petrobras.com.br>>\n",
        "- Antonio de Padua Pires <<antonio.pires@petrobras.com.br>>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f7f52bf",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T22:45:25.170702Z",
          "start_time": "2023-01-23T22:45:20.529246Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-24T00:33:21.459984Z",
          "iopub.status.busy": "2025-09-24T00:33:21.459582Z",
          "iopub.status.idle": "2025-09-24T00:33:30.382858Z",
          "shell.execute_reply": "2025-09-24T00:33:30.381085Z",
          "shell.execute_reply.started": "2025-09-24T00:33:21.459961Z"
        },
        "id": "3f7f52bf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn.datasets\n",
        "import sklearn.preprocessing as skp\n",
        "import sklearn.pipeline as skpp\n",
        "import sklearn.linear_model as skl\n",
        "import sklearn.metrics as skm\n",
        "import sklearn.model_selection as skms\n",
        "import matplotlib.colors as clrs\n",
        "import warnings\n",
        "from IPython.display import display_html\n",
        "\n",
        "cmap = clrs.LinearSegmentedColormap.from_list('train',['green','white','orange'])\n",
        "cmap_val = clrs.LinearSegmentedColormap.from_list('validation',['lime','white','red'])\n",
        "palette = ['green','orange']\n",
        "palette_val = ['lime','red']\n",
        "\n",
        "def show_2D_model(pipe,X,y,ngrid=61, ax=None, cmap=cmap, make_3d=False, marker='o', clb=True):\n",
        "    X_arr = np.asarray(X)\n",
        "    y_arr = np.asarray(y)\n",
        "    if ax is None:\n",
        "        if make_3d:\n",
        "            fig = plt.figure()\n",
        "            ax = fig.add_subplot(1,1,projection='3d')\n",
        "        else:\n",
        "            ax = plt.gca()\n",
        "    # Defining the grid\n",
        "    xs_grid = [np.linspace(x.min()-x.std(),x.max()+x.std(),ngrid+i) for (i,x) in enumerate(X_arr.T)]\n",
        "    Xs_grid = np.meshgrid(*xs_grid)\n",
        "    X_grid = np.array(Xs_grid).reshape(2,-1).T\n",
        "\n",
        "    cols = X.columns if hasattr(X,'columns') else None\n",
        "\n",
        "    df_grid = pd.DataFrame(X_grid,columns=cols) # avoids warning when preprocessing steps have feature names\n",
        "\n",
        "    y_hat_grid = pipe.predict_proba(df_grid)[:,1].reshape(Xs_grid[0].shape)\n",
        "    if make_3d:\n",
        "        ax.scatter(*X_arr.T,y_arr,c=y_arr,marker=marker,cmap=cmap)\n",
        "    else:\n",
        "        ax.scatter(*X_arr.T,c=y_arr,marker=marker,cmap=cmap)\n",
        "\n",
        "    if make_3d:\n",
        "        CS = ax.plot_surface(*Xs_grid,y_hat_grid,cmap=cmap,alpha=0.4,vmin=0,vmax=1)\n",
        "    else:\n",
        "        CS = ax.contourf(*Xs_grid,y_hat_grid,9,cmap=cmap,alpha=0.4,vmin=0,vmax=1)\n",
        "    ax.contour(*Xs_grid,y_hat_grid,[0.5],colors='k',linestyles='--')\n",
        "    ax.grid(); ax.set_xlabel(df_grid.columns[0]); ax.set_ylabel(df_grid.columns[1])\n",
        "    if clb:\n",
        "        plt.colorbar(CS,ax=ax)\n",
        "    return ax\n",
        "\n",
        "def show_1D_points(x_train,y_train,x_val,y_val,ax=None):\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    h = ax.scatter(x_train,y_train,c=y_train,cmap=cmap)\n",
        "    plt.colorbar(h,ax=ax)\n",
        "    ax.scatter(x_val  ,y_val  ,c=y_val,cmap=cmap_val,marker='x')\n",
        "    return ax\n",
        "\n",
        "def compare_perf(py1_train,py1_validation,py2_train,py2_validation,y_train,y_validation,titles):\n",
        "    fig,axs = plt.subplots(2,4,sharey='row', figsize=(9,4))\n",
        "    strip_opts = {'jitter':0.3,'alpha':0.4,'legend':False,'orient':'h'}\n",
        "    sns.stripplot(x=py1_train[:,1],y=y_train,ax=axs[0,0],\n",
        "                hue=y_train,palette=palette,**strip_opts)\n",
        "    sns.stripplot(x=py1_validation[:,1],y=y_validation,ax=axs[0,1],\n",
        "                hue=y_validation,palette=palette_val,**strip_opts,marker='X',s=8)\n",
        "    sns.stripplot(x=py2_train[:,1],y=y_train,ax=axs[0,2],\n",
        "                hue=y_train,palette=palette,**strip_opts)\n",
        "    sns.stripplot(x=py2_validation[:,1],y=y_validation,ax=axs[0,3],\n",
        "                hue=y_validation,palette=palette_val,**strip_opts,marker='X',s=8)\n",
        "    [ax.axvline(x=0.5, color='k', linestyle='--') for ax in axs[0]]\n",
        "    [ax.set_xlabel('p (sandstone)') for ax in axs[0]]\n",
        "    axs[0,0].set_ylabel('True facies')\n",
        "    [ax.set_title(title) for (ax,title) in zip(axs[0],titles)]\n",
        "\n",
        "    cm_opts = {'fmt':'d','xticklabels':[0,1],'yticklabels':[0,1],'cbar':False,\n",
        "            'annot':True,'annot_kws':{\"size\": 14}}\n",
        "    cm1_train = skm.confusion_matrix(y_train, py1_train[:,1]>0.5)\n",
        "    cm1_val = skm.confusion_matrix(y_validation, py1_validation[:,1]>0.5)\n",
        "    cm2_train = skm.confusion_matrix(y_train, py2_train[:,1]>0.5)\n",
        "    cm2_val = skm.confusion_matrix(y_validation, py2_validation[:,1]>0.5)\n",
        "    sns.heatmap(cm1_train, cmap='Blues', ax=axs[1,0],**cm_opts)\n",
        "    sns.heatmap(cm1_val  , cmap='Reds' , ax=axs[1,1],**cm_opts)\n",
        "    sns.heatmap(cm2_train, cmap='Blues', ax=axs[1,2],**cm_opts)\n",
        "    sns.heatmap(cm2_val  , cmap='Reds' , ax=axs[1,3],**cm_opts)\n",
        "    [ax.set_xlabel('Predicted facies') for ax in axs[1]]\n",
        "    axs[1,0].set_ylabel('True facies')\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    cms = [skm.classification_report(y, py[:,1]>0.5,output_dict=True) for (y,py) in zip([y_train,y_validation],[py1_train,py1_validation])] + \\\n",
        "          [skm.classification_report(y, py[:,1]>0.5,output_dict=True) for (y,py) in zip([y_train,y_validation],[py2_train,py2_validation])]\n",
        "    cms = [pd.DataFrame(cm).transpose() for cm in cms]\n",
        "    stylers = [df.style.set_table_attributes(\"style='display:inline'\").set_caption(title).format(\"{:.2f}\") for (df,title) in zip(cms,titles)]\n",
        "\n",
        "    print()\n",
        "    display_html(''.join([styler._repr_html_() for styler in stylers[:2]]), raw=True)\n",
        "    print()\n",
        "    display_html(''.join([styler._repr_html_() for styler in stylers[2:]]), raw=True)\n",
        "\n",
        "    return fig,axs,cms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f5963e",
      "metadata": {
        "id": "67f5963e"
      },
      "source": [
        "## Synthetic dataset\n",
        "\n",
        "Consider the following dataset, which qualitatively simulates 2 **features** with very different scales (**GR** and **NPHI**), which can be used to classify each example as belonging to facies 1 (sandstone), or 0 (shale).\n",
        "\n",
        "We will reserve 70% of the dataset for training and 30% for validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a1c3c7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T22:48:44.828329Z",
          "start_time": "2023-01-23T22:48:44.807310Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-24T00:33:52.208954Z",
          "iopub.status.busy": "2025-09-24T00:33:52.208457Z",
          "iopub.status.idle": "2025-09-24T00:33:52.231687Z",
          "shell.execute_reply": "2025-09-24T00:33:52.230767Z",
          "shell.execute_reply.started": "2025-09-24T00:33:52.208926Z"
        },
        "id": "b2a1c3c7"
      },
      "outputs": [],
      "source": [
        "X,y = sklearn.datasets.make_moons(n_samples=(150,180),noise=0.25,random_state=42)\n",
        "y = 1-y\n",
        "X[:,0] = (X[:,0] + 1.5)/7\n",
        "X[:,1] = -X[:,1]*30 + 50\n",
        "\n",
        "df = pd.DataFrame(np.column_stack((X,y)), columns=['NPHI', 'GR', 'FACIES'])\n",
        "\n",
        "df_train,df_validation = skms.train_test_split(df,train_size=0.7,random_state=23)\n",
        "print(f'{df_train.shape = } ;  {df_validation.shape = }')\n",
        "df.sample(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ffeb4cc-996c-4b63-9f1c-e8c96d3d775d",
      "metadata": {
        "id": "3ffeb4cc-996c-4b63-9f1c-e8c96d3d775d"
      },
      "source": [
        "Notice how neither feature by itself can unambiguously identify the correct facies, but both together (almost) can:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9aab50f-943d-494a-8f04-2912d5ce74b1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T00:35:14.378621Z",
          "iopub.status.busy": "2025-09-24T00:35:14.378134Z",
          "iopub.status.idle": "2025-09-24T00:35:16.214006Z",
          "shell.execute_reply": "2025-09-24T00:35:16.212202Z",
          "shell.execute_reply.started": "2025-09-24T00:35:14.378592Z"
        },
        "id": "c9aab50f-943d-494a-8f04-2912d5ce74b1"
      },
      "outputs": [],
      "source": [
        "px = sns.pairplot(data=df_train,x_vars=df.columns[:-1],y_vars=['FACIES'], hue='FACIES',\n",
        "                 plot_kws={'alpha':0.5},palette=palette)\n",
        "px.axes[0,0].scatter(df_validation['NPHI'],df_validation['FACIES'],c=df_validation['FACIES'],cmap=cmap_val,marker='x',alpha=0.5)\n",
        "px.axes[0,1].scatter(df_validation['GR'],df_validation['FACIES'],c=df_validation['FACIES'],cmap=cmap_val,marker='x',alpha=0.5,label='validation')\n",
        "px.axes[0,1].legend()\n",
        "\n",
        "jx = sns.jointplot(data=df_train,x='GR', y='NPHI',hue='FACIES',height=4,palette=palette)\n",
        "jx.ax_joint.scatter(df_validation['GR'], df_validation['NPHI'],c=df_validation['FACIES'],cmap=cmap_val,marker='x',label='validation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "441a2524-dae8-4381-ac22-da2d145ae4c3",
      "metadata": {
        "id": "441a2524-dae8-4381-ac22-da2d145ae4c3"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "We will use a similar strategy to linear regression to estimate the probability $p_{i}$ that example $i$ belongs to the positive class, based on a linear combination of the input **features**. This simple linear combination does not represent a probability, since it can be any number in $(-\\infty, \\infty)$. It represents a quantity we call the **logit**, or **log-odds**, represented here by $\\ell_i$:\n",
        "\n",
        "$$\n",
        "\\ell_i = \\textrm{logit}(p_i) = \\ln \\left( \\frac{p_i}{1-p_i} \\right) = w_0 + w_1x_{i1} + \\cdots + w_Mx_{iM} = \\vec{w} \\cdot \\vec{x}_i\n",
        "$$\n",
        "\n",
        "where, same as previously, it is common to augment the feature vector with a constant feature of value $1$ and the model parameters are called its *bias* and *weights*.\n",
        "\n",
        "In order to guarantee that the estimated probabilities are between $0$ and $1$, the linear combination will be passed through the **logistic function**, also called **sigmoid** ($\\sigma$):\n",
        "\n",
        "$$\n",
        "p_i = \\sigma\\left(\\ell_i\\right) = \\frac{1}{1+e^{-\\vec{w} \\cdot \\vec{x}_i}} = \\frac{e^{\\vec{w} \\cdot \\vec{x}_i}}{e^{\\vec{w} \\cdot \\vec{x}_i}+1}\n",
        "$$\n",
        "\n",
        "Notice how very negative values of $\\ell_i=\\vec{w} \\cdot \\vec{x}_i$ map to probabilities close to $0$, while very positive values map to probabilities close to $1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2cc98d-5085-41a1-9861-1b0935bb1616",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:04:50.530698Z",
          "iopub.status.busy": "2025-09-24T01:04:50.530003Z",
          "iopub.status.idle": "2025-09-24T01:04:50.726569Z",
          "shell.execute_reply": "2025-09-24T01:04:50.725322Z",
          "shell.execute_reply.started": "2025-09-24T01:04:50.530667Z"
        },
        "id": "9c2cc98d-5085-41a1-9861-1b0935bb1616"
      },
      "outputs": [],
      "source": [
        "ell = np.linspace(-6,6,71)  # many values, just to look at the plot\n",
        "\n",
        "def sigmoid(log_odds):\n",
        "    return ... # YOUR CODE HERE\n",
        "\n",
        "plt.figure(figsize=(6,2))\n",
        "plt.plot(ell,sigmoid(ell))\n",
        "plt.xlabel('logit'); plt.ylabel('logistic'); plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4513f467-31b3-4181-b154-502ec6d23fd2",
      "metadata": {
        "id": "4513f467-31b3-4181-b154-502ec6d23fd2"
      },
      "source": [
        "A high performance model should estimate high probability values $(\\sigma_i \\approx 1)$ for examples belonging to the positive class $(y_i=1)$ and low values otherwise ($\\sigma_i \\approx 0$ , when $y_i=0$). In other words, we want $\\sigma_i \\approx y_i$, similar to the previously studied regression problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beadece0-c386-4219-9b3b-a7f736fbeea7",
      "metadata": {
        "id": "beadece0-c386-4219-9b3b-a7f736fbeea7"
      },
      "source": [
        "# Single feature model\n",
        "\n",
        "In order to gain some intuition, let us explore a simplified model where the only input feature is **GR**, that is:\n",
        "\n",
        "$$\n",
        "p_i = \\sigma(\\ell_i) = \\frac{1}{1+e^{-\\left(w_0 + w_1GR_i \\right)}}\n",
        "$$\n",
        "\n",
        "where training boils down to obtaining good values for the model's **bias** $(w_0)$ and **weight** $(w_1)$.\n",
        "\n",
        "Try it out manually, below. Notice how the **bias** $w_0$ shifts the curve laterally, while the **weight** $w_1$ controls how quickly, and in which direction, the curve flips from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54bac7ba-6405-47d2-a750-7020800b42a7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:05:36.888778Z",
          "iopub.status.busy": "2025-09-24T01:05:36.888419Z",
          "iopub.status.idle": "2025-09-24T01:05:37.269884Z",
          "shell.execute_reply": "2025-09-24T01:05:37.268622Z",
          "shell.execute_reply.started": "2025-09-24T01:05:36.888753Z"
        },
        "id": "54bac7ba-6405-47d2-a750-7020800b42a7"
      },
      "outputs": [],
      "source": [
        "w_0, w_1 = ... , ... # YOUR CODE HERE. Try to estimate good values for the model parameters\n",
        "\n",
        "gr_ax = np.linspace(df['GR'].min()-1,df['GR'].max()+1,91) # GR values that span the whole axis\n",
        "p_ax = sigmoid( ... + ... * gr_ax) # YOUR CODE HERE\n",
        "\n",
        "plt.figure(figsize=(6,2))\n",
        "show_1D_points(df_train['GR'], df_train['FACIES'], df_validation['GR'],df_validation['FACIES'])\n",
        "plt.plot(gr_ax,p_ax)\n",
        "plt.xlabel('GR'); plt.ylabel('p (sandstone)'); plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9babfe-f3bf-44c8-a97e-1d5610fba382",
      "metadata": {
        "id": "cf9babfe-f3bf-44c8-a97e-1d5610fba382"
      },
      "source": [
        "# Z-normalization\n",
        "\n",
        "Notice how sensitive the predicted facies probabilities are to the specific values of the model parameters. One common strategy to improve this is to center and rescale the features, a procedure known as **z-normalization**. For each feature $x_{ij}$, subtract the **mean** $\\mu_j$ and divide the result by the **standard deviation** $s_j$ and use the transformed features $z_{ij}$ as inputs to the model:\n",
        "\n",
        "$$\n",
        "z_{ij} = \\frac{x_{ij}-\\mu_j}{s_j} \\qquad ; \\qquad \\mu_j = \\frac{1}{N}\\sum_{i=1}^N x_{ij} \\qquad ; \\qquad\n",
        "s_j = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left(x_{ij}-\\mu_j\\right)^2}\n",
        "$$\n",
        "\n",
        "**Important!** To avoid data leakage from the validation dataset, the **mean** and **standard deviation** values are calculated only over the **training dataset**. These same values are now part of the ML pipeline and must be stores along with the model for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2bca840-b530-4335-a742-c6a59683a83a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:06:55.677200Z",
          "iopub.status.busy": "2025-09-24T01:06:55.676821Z",
          "iopub.status.idle": "2025-09-24T01:06:55.682609Z",
          "shell.execute_reply": "2025-09-24T01:06:55.681958Z",
          "shell.execute_reply.started": "2025-09-24T01:06:55.677177Z"
        },
        "id": "f2bca840-b530-4335-a742-c6a59683a83a"
      },
      "outputs": [],
      "source": [
        "mu_GR  = df_ ... ['GR'].mean() # YOUR CODE HERE. Obtaining statistics for z-norm\n",
        "std_GR = df_ ... ['GR'].std()  # YOUR CODE HERE. Obtaining statistics for z-norm\n",
        "\n",
        "z_GR_train = (df_train['GR']-mu_GR)/std_GR  # z-norm for the training dataset\n",
        "z_GR_validation = ( ... - ... )/ ... # YOUR CODE HERE. z-norm for the validation dataset\n",
        "\n",
        "z_GR_train = z_GR_train.values # turning into numpy arrays\n",
        "z_GR_validation = z_GR_validation.values\n",
        "y_train = df_train['FACIES'].values\n",
        "y_validation = df_validation['FACIES'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b64ac3-8d83-45b2-a91f-e3eecc3255ee",
      "metadata": {
        "id": "26b64ac3-8d83-45b2-a91f-e3eecc3255ee"
      },
      "source": [
        "Notice how much easier it is to find reasonable parameters with a z-normalized feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0f3365f-e5fa-4ece-b5a9-ca3805764e47",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:06:58.263695Z",
          "iopub.status.busy": "2025-09-24T01:06:58.263364Z",
          "iopub.status.idle": "2025-09-24T01:06:58.757901Z",
          "shell.execute_reply": "2025-09-24T01:06:58.756177Z",
          "shell.execute_reply.started": "2025-09-24T01:06:58.263671Z"
        },
        "id": "a0f3365f-e5fa-4ece-b5a9-ca3805764e47"
      },
      "outputs": [],
      "source": [
        "w_0, w_1 = ... , ... # YOUR CODE HERE. Try to estimate good values for the model parameters\n",
        "\n",
        "z_ax = np.linspace(z_GR_train.min()-1,z_GR_train.max()+1,91) # GR values that span the whole axis\n",
        "\n",
        "p_ax = sigmoid(w_0+w_1*z_ax)\n",
        "plt.figure(figsize=(6,2))\n",
        "show_1D_points(z_GR_train,y_train, z_GR_validation,y_validation)\n",
        "plt.plot(z_ax,p_ax)\n",
        "plt.xlabel('$z_{GR}$'); plt.ylabel('p (sandstone)')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1959f0-924c-4932-8c79-845ddce010a8",
      "metadata": {
        "id": "2a1959f0-924c-4932-8c79-845ddce010a8"
      },
      "source": [
        "# Objective functions and loss landscape\n",
        "\n",
        "A high performance model should output $p_i \\approx y_i$. We express this condition in the form of an **objective function**, which can take on several forms. We will explore 2 examples, with different characteristics: **mean squared error** (MSE) and **binary cross-entropy** (H):\n",
        "\n",
        "$$\\begin{align}\n",
        "MSE(\\vec w)   &= \\phantom{-}\\frac{1}{N}\\sum_{i=1}^N \\left( p_i-y_i \\right)^2 \\\\\n",
        "H(\\vec w)   &=  -         \\frac{1}{N}\\sum_{i=1}^N y_i\\ln{p_i} + (1-y_i)\\ln{(1-p_i)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "For the logistic model, the binary cross-entropy can be written in a mathematically equivalent form which is numerically more stable, since it avoids exponentials with large positive values. However, we use the *logits* $\\ell_i$ instead of the predicted probabilities:\n",
        "\n",
        "$$\n",
        "H = \\dfrac{1}{N}\\sum_i \\max(0,\\ell_i) + \\ln\\left(1+e^{-|\\ell_i|} \\right) - y_i\\ell_i  \\quad ; \\quad p_i = \\sigma(\\ell_i) = \\frac{1}{1+e^{-\\ell_i}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c5a76e-db0c-432d-8ce5-81ccba8b2102",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:08:05.424290Z",
          "iopub.status.busy": "2025-09-24T01:08:05.424037Z",
          "iopub.status.idle": "2025-09-24T01:08:05.428881Z",
          "shell.execute_reply": "2025-09-24T01:08:05.428126Z",
          "shell.execute_reply.started": "2025-09-24T01:08:05.424274Z"
        },
        "id": "00c5a76e-db0c-432d-8ce5-81ccba8b2102"
      },
      "outputs": [],
      "source": [
        "def MSE(p,y, axis=0):\n",
        "    '''Mean Squared error'''\n",
        "    return np.mean( ... , axis=axis) # YOUR CODE HERE\n",
        "\n",
        "def H(p,y, axis=0):\n",
        "    '''Naive binary cross-entropy'''\n",
        "    return -np.mean( y*np.log( ... ) + (1-y)*np.log( ... ) , axis=axis) # YOUR CODE HERE\n",
        "\n",
        "def H_from_logits(logit,y, axis=0):\n",
        "    '''Binary cross entropy (numericaly stable)'''\n",
        "    return np.mean( np.maximum(0,logit) + np.log(1 + np.exp( -np.abs(logit))) - y*logit , axis=axis)\n",
        "\n",
        "p_ax = np.linspace(0.01,0.99,61)  # many values\n",
        "MSE_ax_0 = MSE(p_ax[None,:],y=0); MSE_ax_1 = MSE(p_ax[None,:],y=1)\n",
        "H_ax_0   = H(p_ax[None,:],y=0)  ; H_ax_1   = H(p_ax[None,:],y=1)\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(8,3),sharex='all',sharey='all')\n",
        "axs[0].plot(p_ax,MSE_ax_0,label='y_true=0'); axs[0].plot(p_ax,MSE_ax_1,label='y_true=1')\n",
        "axs[1].plot(p_ax,H_ax_0,label='y_true=0')  ;axs[1].plot(p_ax,H_ax_1,label='y_true=1')\n",
        "axs[0].set_xlabel('p'); axs[0].set_ylabel('Loss'); axs[0].set_title('MSE')\n",
        "axs[1].set_xlabel('p'); axs[1].set_title('H'); axs[0].set_ylim(-0.1,2)\n",
        "[ax.legend() for ax in axs]; [ax.grid() for ax in axs];"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b1d101-e2dc-48b1-9fa8-cf1fa4d013bd",
      "metadata": {
        "id": "70b1d101-e2dc-48b1-9fa8-cf1fa4d013bd"
      },
      "source": [
        "### Loss landscape\n",
        "\n",
        "Since our simplified model has only two trainable parameters, it is possible for us to visualize the loss function over a certain region of the parameter space.\n",
        "\n",
        "Let's plot the $MSE$ and $H$ functions for $w_0 \\in [-20,20]$ and $w_1 \\in [-15,15]$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61cfa9ab-7119-451a-897a-827e3c2af134",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:11:14.858696Z",
          "iopub.status.busy": "2025-09-24T01:11:14.858353Z",
          "iopub.status.idle": "2025-09-24T01:11:16.381040Z",
          "shell.execute_reply": "2025-09-24T01:11:16.379804Z",
          "shell.execute_reply.started": "2025-09-24T01:11:14.858671Z"
        },
        "id": "61cfa9ab-7119-451a-897a-827e3c2af134"
      },
      "outputs": [],
      "source": [
        "w0_grid = np.linspace(-20,20,41)\n",
        "w1_grid = np.linspace(-15,15,43)\n",
        "\n",
        "ww0,ww1 = np.meshgrid(w0_grid,w1_grid)\n",
        "MSE_grid = np.vectorize( lambda w0,w1: MSE(sigmoid(w0+w1*z_GR_train),y_train) )(ww0,ww1) # No loop\n",
        "#H_grid   = np.vectorize( lambda w0,w1: H(  sigmoid(w0+w1*z_GR_train),y_train) )(ww0,ww1) # No loop\n",
        "H_grid   = np.vectorize( lambda w0,w1: H_from_logits(w0+w1*z_GR_train,y_train) )(ww0,ww1) # No loop\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "ax = plt.subplot(1,2,1)\n",
        "plt.contourf(ww0,ww1,MSE_grid,151,cmap='rainbow')\n",
        "plt.colorbar()\n",
        "plt.contour(ww0,ww1,MSE_grid,51,colors='k',linewidths=0.5)\n",
        "plt.xlabel('w0 (bias)'), plt.ylabel('w1 (weight)'), ax.set_aspect(1), plt.title('MSE')\n",
        "\n",
        "ax2 = plt.subplot(1,2,2,sharex=ax,sharey=ax)\n",
        "plt.contourf(ww0,ww1,H_grid,151,cmap='rainbow')\n",
        "plt.colorbar()\n",
        "plt.contour(ww0,ww1,H_grid,51,colors='k',linewidths=0.5)\n",
        "plt.xlabel('w0 (bias)'), plt.ylabel('w1 (weight)'), ax2.set_aspect(1), plt.title('H')\n",
        "plt.axis([w0_grid[0],w0_grid[-1],w1_grid[0],w1_grid[-1]])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a8ca6a-20e3-4487-afac-0ea4ec3ff4ee",
      "metadata": {
        "id": "68a8ca6a-20e3-4487-afac-0ea4ec3ff4ee"
      },
      "source": [
        "# Optimization procedure\n",
        "\n",
        "In order to find the optimum model **parameters** (**train** the model), we will make use of the **gradient descent** method, which consists of updating an initial estimate of the model weights acording to the rule:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\vec{w}^{(1)} & =  \\vec{w}^{(0)} - \\gamma \\vec{\\nabla} L(\\vec{w}^{(0)}) \\\\\n",
        "\\vec{w}^{(2)} & =  \\vec{w}^{(1)} - \\gamma \\vec{\\nabla} L(\\vec{w}^{(1)}) \\\\\n",
        "& \\, \\,  \\vdots \\\\\n",
        "\\vec{w}^{(n)} & =  \\vec{w}^{(n-1)} - \\gamma \\vec{\\nabla} L(\\vec{w}^{(n-1)})\n",
        "\\end{align}\n",
        "$$\n",
        "repeat until $\\lVert \\vec{\\nabla} L(\\vec{w}^{(n)}) \\rVert < tol$\n",
        "\n",
        "where $\\vec{w}^{(0)}$ is an initial guess of the parameters vector, $\\vec{w}^{(n)}$ is the updated parameters vector after the $n$-th iteration, $\\vec{\\nabla}L$ is the **gradient** of the loss function with respect to the model parameters and $\\gamma$ is a **hyperparameter** called the **learning rate**, which is basically a **step-size multiplier**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1157c883-1f8d-436f-af3e-a35261c7d63a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:14:13.108439Z",
          "iopub.status.busy": "2025-09-24T01:14:13.108174Z",
          "iopub.status.idle": "2025-09-24T01:14:13.112924Z",
          "shell.execute_reply": "2025-09-24T01:14:13.112194Z",
          "shell.execute_reply.started": "2025-09-24T01:14:13.108423Z"
        },
        "id": "1157c883-1f8d-436f-af3e-a35261c7d63a"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(f_grad,w,args=(),gamma=1e-2,tol=1e-5,maxiter=2000):\n",
        "    ''' Gradient descent for multivariate optimization.\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "        f_grad: function that returns the objective function gradient. f_grad(w, *args) -> float array\n",
        "        w: initial estimate for the optimum parameters\n",
        "        args: extra arguments for f_grad. (Optional)\n",
        "        gamma: step size multiplier\n",
        "        tol: optimization stops when ||f_grad(w)|| < tol\n",
        "        maxiter: maximum number of iterations\n",
        "\n",
        "    Outputs:\n",
        "    -----------\n",
        "        w_opt: parameterr that minimizes function\n",
        "        history: np.array that stores all steps taken\n",
        "    '''\n",
        "    grad = f_grad(w,*args) # Gradient at starting point\n",
        "    history = [w]\n",
        "    for it in range(int(maxiter)):\n",
        "        w = w - gamma*grad  # Update rule\n",
        "        grad = f_grad(w,*args)  # new gradient\n",
        "        history.append(w)\n",
        "\n",
        "        if (np.linalg.norm(grad)<tol): # halting criterion\n",
        "            break\n",
        "    else: # only runs if the loop exhausts range(maxiter) wihtout triggering the break command\n",
        "        warnings.warn(f'Did not converge after {it+1} iterations. Try different parameters.')\n",
        "\n",
        "    return w, np.array(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3760f80c-9511-401a-9385-5f53f977dc1c",
      "metadata": {
        "id": "3760f80c-9511-401a-9385-5f53f977dc1c"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "Writing each loss function explicitly in terms of the parameters:\n",
        "\n",
        "$$\\begin{align}\n",
        "MSE(\\vec w)   &= \\phantom{-}\\frac{1}{N}\\sum_{i=1}^N \\Bigl[ \\sigma(\\overbrace{w_0+w_1x_i}^{\\ell_i})-y_i \\Bigr]^2 \\\\\n",
        "H(\\vec w)   &=  -         \\frac{1}{N}\\sum_{i=1}^N y_i\\ln{\\Bigl[\\sigma(w_0+w_1x_i)\\Bigr]} + (1-y_i)\\ln{\\Bigl[1-\\sigma(w_0+w_1x_i)\\Bigr]}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Each partial derivative can be found with a repeated application fo the chain rule:\n",
        "\n",
        "For MSE:\n",
        "$$\\begin{align}\n",
        "\\frac{\\partial MSE}{\\partial w_j} &= \\frac{1}{N}\\sum_i 2\\Bigl[ \\sigma(\\ell_i)-y_i \\Bigr]\n",
        "\\quad \\frac{d \\sigma(\\ell_i)}{d \\ell_i} \\quad\n",
        "\\frac{\\partial \\ell_i}{\\partial w_j} \\\\\n",
        "\\frac{\\partial MSE}{\\partial w_j} &=\n",
        "\\frac{1}{N}\\sum_i 2\\Bigl[ \\sigma(\\ell_i)-y_i \\Bigr]\n",
        "\\sigma(\\ell_i) \\Bigl[ 1 - \\sigma(\\ell_i) \\Bigr]\n",
        "x_{ij} \\end{align}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial MSE}{\\partial w_j} =  \n",
        "\\frac{1}{N}\\sum_i 2 \\, (p_i-y_i) \\, \\, p_i \\, \\,  (1 - p_i) \\, x_{ij}\n",
        "$$\n",
        "\n",
        "For the binary cross-entropy:\n",
        "\n",
        "$$\\begin{align}\n",
        "\\frac{\\partial H}{\\partial w_j} &= -\\frac{1}{N}\\sum_i\n",
        "y_i \\frac{1}{\\sigma(\\ell_i)}\n",
        "\\quad \\frac{d \\sigma(\\ell_i)}{d \\ell_i} \\quad\n",
        "\\frac{\\partial \\ell_i}{\\partial w_j} \\quad+\n",
        "(1-y_i) \\frac{(-1)}{1-\\sigma(\\ell_i)}\n",
        "\\quad \\frac{d \\sigma(\\ell_i)}{d \\ell_i} \\quad\n",
        "\\frac{\\partial \\ell_i}{\\partial w_j} \\\\\n",
        "\\frac{\\partial H}{\\partial w_j} &= -\\frac{1}{N}\\sum_i\n",
        "y_i \\frac{1}{\\sigma(\\ell_i)}\n",
        "\\sigma(\\ell_i) \\Bigl[1-\\sigma(\\ell_i) \\Bigr]\n",
        "x_{ij} -\n",
        "(1-y_i) \\frac{1}{1-\\sigma(\\ell_i)}\n",
        "\\sigma(\\ell_i) \\Bigl[1-\\sigma(\\ell_i) \\Bigr]\n",
        "x_{ij}  \\end{align}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial H}{\\partial w_j} = \\frac{1}{N}\\sum_i (p_i-y_i)x_{ij}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1830329e-f05c-4fcc-9855-f1105fd454aa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:17:00.895933Z",
          "iopub.status.busy": "2025-09-24T01:17:00.895455Z",
          "iopub.status.idle": "2025-09-24T01:17:00.902607Z",
          "shell.execute_reply": "2025-09-24T01:17:00.901867Z",
          "shell.execute_reply.started": "2025-09-24T01:17:00.895903Z"
        },
        "id": "1830329e-f05c-4fcc-9855-f1105fd454aa"
      },
      "outputs": [],
      "source": [
        "def grad_MSE(w,x,y):\n",
        "    '''Gradient for MSE of logistic function with a single feature'''\n",
        "    w0,w1 = w\n",
        "    p = sigmoid(w0+w1*x)\n",
        "    dMSE_dw0 = np.mean( 2*(p-y)*p*(1-p)   )\n",
        "    dMSE_dw1 = np.mean( ... ) # YOUR CODE HERE\n",
        "    return np.array([dMSE_dw0, dMSE_dw1])\n",
        "\n",
        "def grad_H(w,x,y):\n",
        "    '''Gradient for cross-entropy of logistic function with a single feature'''\n",
        "    w0,w1 = w\n",
        "    p = sigmoid(w0+w1*x)\n",
        "    dMSE_dw0 = np.mean( ... ) # YOUR CODE HERE\n",
        "    dMSE_dw1 = np.mean( ... ) # YOUR CODE HERE\n",
        "    return np.array([dMSE_dw0, dMSE_dw1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc0322d3-f207-4021-9a23-d8e2fa182cbb",
      "metadata": {
        "id": "fc0322d3-f207-4021-9a23-d8e2fa182cbb"
      },
      "source": [
        "Try out a few optimization runs with different parameters. Notice how the binary cross-entropy loss function, with its single well defined minimum, is much easier to optimize. Contrast with MSE and its flat regions, where $\\vec{\\nabla} L$ is close to zero, such that gradient descent either halts prematurely, or makes only **very** slow progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f9274b-c46b-4a0b-a7f1-c4fbbf7cf632",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:18:45.382389Z",
          "iopub.status.busy": "2025-09-24T01:18:45.381791Z",
          "iopub.status.idle": "2025-09-24T01:18:47.407063Z",
          "shell.execute_reply": "2025-09-24T01:18:47.404904Z",
          "shell.execute_reply.started": "2025-09-24T01:18:45.382363Z"
        },
        "id": "40f9274b-c46b-4a0b-a7f1-c4fbbf7cf632"
      },
      "outputs": [],
      "source": [
        "w_0, w_1 = ... , ... # YOUR CODE HERE. Try to estimate good values for the model parameters\n",
        "# w_0, w_1 = -5,-10  # Alternative guesses\n",
        "# w_0, w_1 = -10, 10  # Alternative guesses\n",
        "gamma   = 10 # learning rate\n",
        "tol     = 1e-4 # numerical tolerance for \"almost zero\" gradient. Optimization halts if ||grad||<tol\n",
        "maxiter = 200 # maximum number of iterations\n",
        "\n",
        "(w0_MSE,w1_MSE),hist_MSE = gradient_descent(grad_MSE,(w_0,w_1),(z_GR_train,y_train),gamma=gamma, tol=tol, maxiter=maxiter)\n",
        "(w0_H,w1_H),hist_H = gradient_descent(grad_H,(w_0,w_1),(z_GR_train,y_train), gamma=gamma/10, tol=tol, maxiter=maxiter)\n",
        "\n",
        "if True:\n",
        "    w0_grid = np.linspace(-15,10,41)\n",
        "    w1_grid = np.linspace(-12,18,43)\n",
        "\n",
        "    ww0,ww1 = np.meshgrid(w0_grid,w1_grid)\n",
        "    MSE_grid = np.vectorize( lambda w0,w1: MSE(sigmoid(w0+w1*z_GR_train),y_train) )(ww0,ww1) # No loop\n",
        "    #H_grid   = np.vectorize( lambda w0,w1: H(  sigmoid(w0+w1*z_GR_train),y_train) )(ww0,ww1) # No loop\n",
        "    H_grid   = np.vectorize( lambda w0,w1: H_from_logits(w0+w1*z_GR_train,y_train) )(ww0,ww1) # No loop\n",
        "\n",
        "    plt.figure(figsize=(12,3.5))\n",
        "    plt.subplot(1,3,1)\n",
        "    show_1D_points(z_GR_train,y_train,z_GR_validation,y_validation)\n",
        "    plt.plot(z_ax,sigmoid(w_0    + w_1*z_ax   ),color='b',label='Initial guess')\n",
        "    plt.plot(z_ax,sigmoid(w0_MSE + w1_MSE*z_ax),color='r',label='MSE fit')\n",
        "    plt.plot(z_ax,sigmoid(w0_H   + w1_H*z_ax  ),color='g',label='H fit')\n",
        "    plt.xlabel('$z_{GR}$'); plt.ylabel('p (sandstone)')\n",
        "    plt.legend(loc='center left'), plt.grid()\n",
        "\n",
        "    ax = plt.subplot(1,3,2)\n",
        "    plt.contourf(ww0,ww1,MSE_grid,151,cmap='rainbow')\n",
        "    plt.colorbar()\n",
        "    plt.contour(ww0,ww1,MSE_grid,51,colors='k',linewidths=0.5)\n",
        "    plt.plot(*hist_MSE.T,'k.:'); plt.plot(w_0, w_1,'bo'); plt.plot(w0_MSE, w1_MSE,'ro')\n",
        "    plt.xlabel('w0 (bias)'), plt.ylabel('w1 (weight)'), ax.set_aspect(1), plt.title('MSE')\n",
        "\n",
        "    ax2 = plt.subplot(1,3,3,sharex=ax,sharey=ax)\n",
        "    plt.contourf(ww0,ww1,H_grid,151,cmap='rainbow')\n",
        "    plt.colorbar()\n",
        "    plt.contour(ww0,ww1,H_grid,51,colors='k',linewidths=0.5)\n",
        "    plt.plot(*hist_H.T,'k.:'); plt.plot(w_0, w_1,'bo'); plt.plot(w0_H, w1_H,'go')\n",
        "    plt.xlabel('w0 (bias)'), plt.ylabel('w1 (weight)'), ax2.set_aspect(1), plt.title('H')\n",
        "    plt.axis([w0_grid[0],w0_grid[-1],w1_grid[0],w1_grid[-1]])\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff666554-0037-41e3-8609-11f98099d883",
      "metadata": {
        "id": "ff666554-0037-41e3-8609-11f98099d883"
      },
      "source": [
        "# Adding more features\n",
        "\n",
        "Now that we have some intuition about how a logistic regression model works, let's train a more capable model that makes use of both **GR** and **NPHI**.\n",
        "\n",
        "## Model pipeline\n",
        "\n",
        "All the following steps will be encapsulated within a `skpp.Pipeline` object, created with `skpp.make_pipeline`.\n",
        "\n",
        "### $z$-normalization\n",
        "\n",
        "This is especially important when dealing with features that have very different scales, such as **GR** (dozens of units) and **NPHI** (fractions of a unit).\n",
        "\n",
        "Available via `skp.StandardScaler`.\n",
        "\n",
        "### Feature engineering\n",
        "\n",
        "Since our dataset is (almost) separable, but not **linearly** separable, it might be useful to create polynomial features with `skp.PolynomialFeatures`. Let's compare a **linear model** and one with **degree 6** polynomial features.\n",
        "\n",
        "### Logistic regression\n",
        "\n",
        "We will use the Scikit-Learn implementation available via `skl.LogisticRegression`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc7b517c-f6a6-4348-87e8-d5439c1c3cef",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:26:32.595198Z",
          "iopub.status.busy": "2025-09-24T01:26:32.594856Z",
          "iopub.status.idle": "2025-09-24T01:26:32.608583Z",
          "shell.execute_reply": "2025-09-24T01:26:32.607609Z",
          "shell.execute_reply.started": "2025-09-24T01:26:32.595173Z"
        },
        "id": "dc7b517c-f6a6-4348-87e8-d5439c1c3cef"
      },
      "outputs": [],
      "source": [
        "pipe1 = skpp.make_pipeline( ... , # YOUR CODE HERE.  Z-norm\n",
        "                            ... (penalty=None,max_iter=1_000) ) # YOUR CODE HERE. Logistic regression. penalty=None,max_iter=1_000\n",
        "\n",
        "pipe2 = skpp.make_pipeline( ... , # YOUR CODE HERE.  Z-norm\n",
        "                            ... (degree=6,include_bias=False), # YOUR CODE HERE.  Polynomial features. include_bias=False\n",
        "                            ... (penalty=None,max_iter=1_000) ) # YOUR CODE HERE. Logistic regression. penalty=None,max_iter=1_000\n",
        "display(pipe2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8196a8bb-1a1c-456a-8cc1-0cfe36f725d0",
      "metadata": {
        "id": "8196a8bb-1a1c-456a-8cc1-0cfe36f725d0"
      },
      "source": [
        "### Training and inference\n",
        "\n",
        "Training is done with `pipe.fit` and inference with `pipe.predict_proba` (returns proabilities for each facies), or `pipe.predict` (returns the most likely facies)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47bd05b9-187e-4523-aed7-ad83ea1dc461",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:26:33.465498Z",
          "iopub.status.busy": "2025-09-24T01:26:33.464969Z",
          "iopub.status.idle": "2025-09-24T01:26:33.773174Z",
          "shell.execute_reply": "2025-09-24T01:26:33.771551Z",
          "shell.execute_reply.started": "2025-09-24T01:26:33.465466Z"
        },
        "id": "47bd05b9-187e-4523-aed7-ad83ea1dc461"
      },
      "outputs": [],
      "source": [
        "features = ['GR', 'NPHI']\n",
        "X_train = df_train[features] # DataFrame with only the features, including their names\n",
        "y_train = df_train['FACIES']\n",
        "\n",
        "X_validation = df_validation[features]\n",
        "y_validation = df_validation['FACIES']\n",
        "\n",
        "pipe1. ... ( ... , ... )  # YOUR CODE HERE. Training the linear model\n",
        "pipe2.fit(X_train,y_train)  # YOUR CODE HERE. Training the model with polynomial features\n",
        "\n",
        "py1_train = pipe1. ... ( ... ) # INFERENCE with the linear model\n",
        "py2_train = pipe2.predict_proba(X_train) # INFERENCE with the polynomial features model\n",
        "py1_validation = pipe1. ... ( ... )\n",
        "py2_validation = pipe2.predict_proba(X_validation)\n",
        "\n",
        "pd.DataFrame(np.column_stack([py1_validation, py2_validation, y_validation]),\n",
        "             columns=['Model 1 Prob class 0', 'Model 1 Prob class 1',\n",
        "                      'Model 2 Prob class 0', 'Model 2 Prob class 1','True Facies']).sample(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d841dff9-b668-41dc-a40f-a94c73eac3d3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:26:35.842700Z",
          "iopub.status.busy": "2025-09-24T01:26:35.842063Z",
          "iopub.status.idle": "2025-09-24T01:26:36.702822Z",
          "shell.execute_reply": "2025-09-24T01:26:36.701180Z",
          "shell.execute_reply.started": "2025-09-24T01:26:35.842670Z"
        },
        "id": "d841dff9-b668-41dc-a40f-a94c73eac3d3"
      },
      "outputs": [],
      "source": [
        "fig,axs = plt.subplots(1,2,sharex='all',sharey='all', figsize=(12,4))\n",
        "\n",
        "show_2D_model(pipe1,X=X_train, y=y_train, ax=axs[0])\n",
        "axs[0].scatter(X_validation['GR'],X_validation['NPHI'],c=y_validation, cmap=cmap_val, marker='x')\n",
        "axs[0].set_title('Model 1')\n",
        "\n",
        "show_2D_model(pipe2,X=X_train, y=y_train, ax=axs[1])\n",
        "axs[1].scatter(X_validation['GR'],X_validation['NPHI'],c=y_validation, cmap=cmap_val, marker='x')\n",
        "axs[1].set_title('Model 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If our model had more than two features, the previous visualization would not be possible. However, we can always check how the model's predictions and the ground truth labels compare. A few common ways of doing this are illustrated below, along with the most common metrics:"
      ],
      "metadata": {
        "id": "abIHc0VhR83J"
      },
      "id": "abIHc0VhR83J"
    },
    {
      "cell_type": "code",
      "source": [
        "titles = ['Model 1\\nTrain','Model 1\\nValidation','Model 2\\nTrain','Model 2\\nValidation']\n",
        "\n",
        "compare_perf(py1_train,py1_validation,py2_train,py2_validation,y_train,y_validation,titles);"
      ],
      "metadata": {
        "id": "bwe8na6cR72S"
      },
      "id": "bwe8na6cR72S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3b6d76a3-d5c6-40b0-aeb8-8ce6163e2125",
      "metadata": {
        "id": "3b6d76a3-d5c6-40b0-aeb8-8ce6163e2125"
      },
      "source": [
        "# Under and Overfitting\n",
        "\n",
        "Notice how the linear model **underfits** the data. It is not sufficiently flexible to conform to our complicated dataset. We say it is a **high bias** model. This can be solved by using a more flexible model, such as one with higher degree polynomial features.\n",
        "\n",
        "On the other hand, the model with polynomial features of degree 6 **overfits** the data. It is a model with **high variance**. Notice also how the model's weights are numerically large."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Model 1 weights: {np.round(pipe1[\"logisticregression\"].coef_, 2)}')\n",
        "print(f'Model 2 weights: {np.round(pipe2[\"logisticregression\"].coef_, 2)}')"
      ],
      "metadata": {
        "id": "Pfq39vZ-NOhM"
      },
      "id": "Pfq39vZ-NOhM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization\n",
        "\n",
        "One strategy for mitigating overfitting is called **regularization**, that is, changing the objective function to account for more than just the data **misfit**, but also penalizing some other characteristic of a complex model. In our case, we'll penalize **large weights**.\n",
        "\n",
        "The objective function that `skl.LogisticRegression` optimizes is the following:\n",
        "\n",
        "$$\n",
        "L = \\underbrace{\\sum_{i=1}^N\\Bigl( y_i\\ln(p_i) + (1-y_i)\\ln(1-p_i) \\Bigr)}_{\\textrm{misfit}} + \\frac{1}{C}\\underbrace{\\lVert \\vec{w} \\rVert^2}_{\\textrm{\"complexity\"}}\n",
        "$$\n",
        "\n",
        "where $C$ is the **regularization hyperparameter**. Large values of $C$ lead to an objective funtion that cares only about the model's misfit (low regularization regime, high variance, low bias), while low values of $C$ lead to an objective function that heavily penalizes large values for the weights (high regularization regime, high bias, low variance).\n",
        "\n",
        "As with other hyperparameters, the best value for $C$ is not found by optimizing the objective function, but rather by estimating which value will lead to the best **performance** when the model is presented with **novel data**. In our workflow, the **validation dataset** is a proxy for this unseen data.\n",
        "\n",
        "The default performance metric for `skl.LogisticRegression` is the model's **accuracy**, that is, the fraction of correct predictions over a given dataset.\n",
        "\n",
        "We will test 16 values of $C$ ranging from $10^{-2}$ to $10^3$:"
      ],
      "metadata": {
        "id": "CtCrvhH1NO3j"
      },
      "id": "CtCrvhH1NO3j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff059e3e-40f9-49d6-880e-4ee99fe137ba",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:36:06.362747Z",
          "iopub.status.busy": "2025-09-24T01:36:06.362292Z",
          "iopub.status.idle": "2025-09-24T01:36:07.934018Z",
          "shell.execute_reply": "2025-09-24T01:36:07.932011Z",
          "shell.execute_reply.started": "2025-09-24T01:36:06.362719Z"
        },
        "id": "ff059e3e-40f9-49d6-880e-4ee99fe137ba"
      },
      "outputs": [],
      "source": [
        "n_Cs = 16  # number of different values to try\n",
        "Cs = np.logspace(-2,3,n_Cs) # C changes by several orders of magnitude\n",
        "\n",
        "pipes = [None]*n_Cs # inicializing with the correct size\n",
        "accuracy_train = np.zeros(n_Cs)\n",
        "accuracy_validation = np.zeros(n_Cs)\n",
        "\n",
        "for i,C in enumerate(Cs):\n",
        "    pipes[i] = skpp.make_pipeline( ...                                , # YOUR CODE HERE for z-normalization\n",
        "                                  skp.PolynomialFeatures(degree= ... ,include_bias=False), # YOUR CODE HERE for polynomial features\n",
        "                                  skl.LogisticRegression(C= ... , max_iter=10_000)       ) # YOUR CODE HERE for classifier. max_iter=10_000\n",
        "    pipes[i].fit(X_train,y_train) # YOUR CODE HERE to train the i-th model\n",
        "\n",
        "    accuracy_train[i] = pipes[i].score(X_train,y_train)\n",
        "    accuracy_validation[i] = pipes[i].score(X_validation,y_validation)\n",
        "\n",
        "best_idx = np.argmax( ... ) # YOUR CODE HERE. Which dataset helps us choose the best C?\n",
        "best_pipe = pipes[best_idx]\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.semilogx(Cs,accuracy_train,'o-',label='training')\n",
        "plt.semilogx(Cs,accuracy_validation,'o-',label='validation')\n",
        "plt.plot(Cs[best_idx],accuracy_validation[best_idx],'xk',label=f'$C_{{opt}}={Cs[best_idx]:.2f}$')\n",
        "plt.xlabel('C (regularization)'), plt.ylabel('Accuracy'), plt.legend()\n",
        "plt.grid(which='both')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c997ba7-da4e-4a1e-ae3f-0427de5d2262",
      "metadata": {
        "id": "1c997ba7-da4e-4a1e-ae3f-0427de5d2262"
      },
      "source": [
        "### Visualizing the regularized models\n",
        "\n",
        "Since the trained models use only 2 features, we can visualize them in feature space. Notice how over-regularized model **underfit** the data, while under-regularized models **overfit** the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed22007c-2427-4757-b145-be02dea15e12",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-24T01:38:00.456260Z",
          "iopub.status.busy": "2025-09-24T01:38:00.455908Z",
          "iopub.status.idle": "2025-09-24T01:38:07.215219Z",
          "shell.execute_reply": "2025-09-24T01:38:07.214156Z",
          "shell.execute_reply.started": "2025-09-24T01:38:00.456236Z"
        },
        "id": "ed22007c-2427-4757-b145-be02dea15e12"
      },
      "outputs": [],
      "source": [
        "nlin = int(np.sqrt(n_Cs))\n",
        "ncol = int(np.ceil(n_Cs/nlin))\n",
        "fig,axs = plt.subplots(nlin,ncol,figsize=(12,12),sharex='all',sharey='all')\n",
        "axs = axs.ravel()\n",
        "for idx,(C,ax,pipe,ACC_t,ACC_v) in enumerate(zip(Cs,axs,pipes,accuracy_train,accuracy_validation)):\n",
        "    show_2D_model(pipe,X=X_train, y=y_train, ax=ax, clb=False)\n",
        "    ax.scatter(X_validation['GR'],X_validation['NPHI'],c=y_validation, cmap=cmap_val, marker='x')\n",
        "    ax.set_title(f'$C={C:.2e}$')\n",
        "    if idx == best_idx:\n",
        "        ax.spines[:].set_color('b')\n",
        "        ax.spines[:].set_linewidth(4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the regularized model's weights are considerably smaller than those of the unregularized model, while maintaining high performance on the training dataset **and better** performance on the validation dataset:"
      ],
      "metadata": {
        "id": "UOsZUJEzYI7P"
      },
      "id": "UOsZUJEzYI7P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b4f0f6",
      "metadata": {
        "id": "11b4f0f6"
      },
      "outputs": [],
      "source": [
        "print('Unregularized model weights:')\n",
        "print(pipe2['logisticregression'].coef_)\n",
        "print()\n",
        "print('Best regularized model weights:')\n",
        "print(best_pipe['logisticregression'].coef_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "py_best_train = best_pipe.predict_proba(X_train)\n",
        "py_best_validation = best_pipe.predict_proba(X_validation)\n",
        "final_titles = ['Unregularized Model\\nTrain','Unregularized Model\\nValidation',\n",
        "                'Regularized Model\\nTrain','Regularized Model\\nValidation']\n",
        "compare_perf(py2_train,py2_validation,py_best_train,py_best_validation,y_train,y_validation,final_titles);"
      ],
      "metadata": {
        "id": "nd2EuIa6zAus"
      },
      "id": "nd2EuIa6zAus",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320fbd5a",
      "metadata": {
        "id": "320fbd5a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af97b00a",
      "metadata": {
        "id": "af97b00a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6eff4bf",
      "metadata": {
        "id": "a6eff4bf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}