{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multilinear Regression\n",
        "## ICE 2025 - Artificial Intelligence for O&G Without the Hype\n",
        "\n",
        ">  - Complete: <a href=\"https://colab.research.google.com/github/paduapires/aapg_ice/blob/main/Multilinear_regression_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        ">\n",
        ">  - Class: <a href=\"https://colab.research.google.com/github/paduapires/aapg_ice/blob/main/Multilinear_regression_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "- Pedro Pesce <<pedro_pesce@petrobras.com.br>>\n",
        "- Thiago Toribio <<thiago.toribio@petrobras.com.br>>\n",
        "- Cesar Calderon <<cjcalderon@petrobras.com.br>>\n",
        "- Luiz Eduardo Queiroz <<eduardoqueiroz@petrobras.com.br>>\n",
        "- Antonio de Padua Pires <<antonio.pires@petrobras.com.br>>"
      ],
      "metadata": {
        "id": "2eG6SXpPZjHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn.preprocessing as skp\n",
        "import sklearn.linear_model as skl\n",
        "import sklearn.metrics as skm\n",
        "import sklearn.pipeline as skpp"
      ],
      "metadata": {
        "id": "21inJqBcZjtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function will be useful for plotting the model results:"
      ],
      "metadata": {
        "id": "uxnKl7gC32KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a colormap for each facies\n",
        "cmap = {'ANHYDRITE':'magenta', 'SANDSTONE':'xkcd:goldenrod', 'CALCARENITE':'cyan',\n",
        "        'CALCILUTITE':'xkcd:dark blue', 'DOLOMITE':'purple', 'SHALE':'green',\n",
        "        'MARL':'xkcd:greenish blue', 'SILTSTONE':'xkcd:reddish orange'}\n",
        "patchList = [mpatches.Patch(color=value, label=key) for (key,value) in cmap.items()]\n",
        "\n",
        "def per_facies_scatter(y_hat,y,facies,xlabel=None,ylabel=None,\n",
        "                       suptitle='',marker='.',linestyle='',cmap=cmap,fig_kwargs=None,plt_kwargs=None):\n",
        "    '''Plots y_hat versus y for each facies'''\n",
        "    n_groups = len(facies_list)  # uses global facies_list\n",
        "    labels = ['All'] + facies_list # uses global facies_list\n",
        "\n",
        "    y_hat = np.asarray(y_hat)\n",
        "    y = np.asarray(y)\n",
        "    facies = np.asarray(facies)\n",
        "\n",
        "    if fig_kwargs is None: fig_kwargs = dict()\n",
        "    if plt_kwargs is None: plt_kwargs = dict()\n",
        "\n",
        "    nlin = int(np.floor(np.sqrt(n_groups+1)))\n",
        "    ncol = int(np.ceil(n_groups/nlin))\n",
        "    fig,axs = plt.subplots(nlin,ncol,sharex='all',sharey='all',\n",
        "                           gridspec_kw={'wspace':0,'hspace':0}, **fig_kwargs)\n",
        "    mM = min([y_hat.min(),y.min()]) , max([y_hat.max(),y.max()])\n",
        "\n",
        "    for i,(ax,label) in enumerate(zip(axs.flat,labels)):\n",
        "        ax.plot(mM,mM,'k--'), ax.grid()\n",
        "        if i == 0: # All points\n",
        "            mask = np.full(y_hat.shape,True)\n",
        "        else:\n",
        "            mask = facies==label\n",
        "        if not mask.any(): # if there are no point in the class, skip it\n",
        "            continue\n",
        "        y_hat_mask,y_mask = y_hat[mask],y[mask]\n",
        "        MSE = ((y_hat_mask-y_mask)**2).mean()\n",
        "        RMSE = np.sqrt(MSE)\n",
        "        R2 = 1 - MSE/y.var()\n",
        "        color = cmap.get(label,'C0')\n",
        "        ax.plot(y_hat_mask,y_mask,marker=marker,linestyle=linestyle,color=color,\n",
        "                label=f'{label}\\n{RMSE=:.2f}\\n{R2=:.3f}',**plt_kwargs)\n",
        "        ax.legend(fontsize=8)\n",
        "    if xlabel:\n",
        "        axs[-1     ,ncol//2].set_xlabel(xlabel)\n",
        "    if ylabel:\n",
        "        axs[nlin//2,0      ].set_ylabel(ylabel)\n",
        "    fig.suptitle(suptitle)\n",
        "    plt.tight_layout()\n",
        "    return fig,axs\n"
      ],
      "metadata": {
        "id": "e-g8hk9V3wEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bmpJBFTSoGi"
      },
      "source": [
        "## Getting the data\n",
        "\n",
        "The file `https://github.com/paduapires/aapg_ice/raw/main/well_logs.xlsx` contains a dataset composed of well log data for 2 wells, with columns for:\n",
        "- Well number: **well**\n",
        "- Depth (m): **z**\n",
        "- Gamma ray ($^\\circ$API): **gr**\n",
        "- Density ($g/cm^3$): **rho**\n",
        "- Neutron porosity (.dec): **nphi**\n",
        "- Compressional slowness ($\\mu s/ft$): **dt_p**\n",
        "- Shear slowness ($\\mu s/ft$): **dt_s**\n",
        "- Facies: **facies**\n",
        "\n",
        "A planned future well will not measure **dt_s**. Our task is to develop a model that estimates **dt_s** (our **target**) from other available data (**features**).\n",
        "\n",
        "Since the task is **predicting** (or **inferring**) a **continuous** value (and not a **categorical** one), this is a **regression** task.\n",
        "\n",
        "Since our models will use available measurements of the target value in order to learn how to map input features to the desired target outputs, this a **supervised** experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-19T20:17:49.470293Z",
          "iopub.status.busy": "2025-09-19T20:17:49.469792Z",
          "iopub.status.idle": "2025-09-19T20:18:01.284057Z",
          "shell.execute_reply": "2025-09-19T20:18:01.282882Z",
          "shell.execute_reply.started": "2025-09-19T20:17:49.470263Z"
        },
        "id": "Ra2fHhOKS2Qo"
      },
      "outputs": [],
      "source": [
        "# Construct the raw file URL for the Excel file on GitHub\n",
        "excel_url = 'https://github.com/paduapires/aapg_ice/raw/main/well_logs.xlsx'\n",
        "\n",
        "# Use pandas to read the Excel file directly from the raw URL\n",
        "df = pd.read_excel(io= ... , engine='openpyxl')\n",
        "display(df.sample(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:54:27.437848Z",
          "start_time": "2023-01-23T16:54:24.371697Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:18:22.895775Z",
          "iopub.status.busy": "2025-09-19T20:18:22.895259Z",
          "iopub.status.idle": "2025-09-19T20:18:22.908604Z",
          "shell.execute_reply": "2025-09-19T20:18:22.907951Z",
          "shell.execute_reply.started": "2025-09-19T20:18:22.895757Z"
        },
        "id": "jGZEfeTESoGj"
      },
      "outputs": [],
      "source": [
        "# eliminating lines with incomplete data\n",
        "df = df. ...  # YOUR CODE HERE to drop lines with NaN\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daiWXcwGZc4S"
      },
      "source": [
        "### Exploratory data analysis\n",
        "\n",
        "Columns `well` and `facies` contain **categorical** data. Counting the ocurrence of each is an easy way to start the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-19T20:18:25.249413Z",
          "iopub.status.busy": "2025-09-19T20:18:25.249027Z",
          "iopub.status.idle": "2025-09-19T20:18:25.267528Z",
          "shell.execute_reply": "2025-09-19T20:18:25.266924Z",
          "shell.execute_reply.started": "2025-09-19T20:18:25.249373Z"
        },
        "id": "VLGTHavXZc4T"
      },
      "outputs": [],
      "source": [
        "df_counts = df[['well','facies']].value_counts().to_frame().transpose()\n",
        "df_counts[df_counts.columns.sort_values()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2HW-EiYSoGk"
      },
      "source": [
        "The `'SAND'` facies has very few examples. Let's merge them into the `'SANDSTONE'` facies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:54:38.986706Z",
          "start_time": "2023-01-23T16:54:38.969705Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:19:55.092999Z",
          "iopub.status.busy": "2025-09-19T20:19:55.092170Z",
          "iopub.status.idle": "2025-09-19T20:19:55.107951Z",
          "shell.execute_reply": "2025-09-19T20:19:55.106815Z",
          "shell.execute_reply.started": "2025-09-19T20:19:55.092962Z"
        },
        "id": "ip_VS77WSoGk"
      },
      "outputs": [],
      "source": [
        "sand_mask = ...  # YOUR CODE HERE to make a logical mask that selects only SAND\n",
        "df.loc[sand_mask,'facies'] = 'SANDSTONE' # changing values in a DataFrame can be finicky...\n",
        "df['facies'].value_counts().to_frame().transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-19T20:19:57.811575Z",
          "iopub.status.busy": "2025-09-19T20:19:57.811224Z",
          "iopub.status.idle": "2025-09-19T20:19:57.818024Z",
          "shell.execute_reply": "2025-09-19T20:19:57.817290Z",
          "shell.execute_reply.started": "2025-09-19T20:19:57.811550Z"
        },
        "id": "2hJeGcajZc4Y"
      },
      "outputs": [],
      "source": [
        "facies_list = list(df['facies'].unique())\n",
        "print(facies_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3shS7mPvZc4b"
      },
      "source": [
        "The other columns contain continuous data. Let's visualize how each **feature** correlates to the **target** variable **dt_s**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:54:55.991002Z",
          "start_time": "2023-01-23T16:54:55.968858Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:20:06.955773Z",
          "iopub.status.busy": "2025-09-19T20:20:06.955525Z",
          "iopub.status.idle": "2025-09-19T20:20:21.863770Z",
          "shell.execute_reply": "2025-09-19T20:20:21.862174Z",
          "shell.execute_reply.started": "2025-09-19T20:20:06.955753Z"
        },
        "id": "IBXk7IW3SoGl"
      },
      "outputs": [],
      "source": [
        "g = sns.pairplot(data=df,x_vars=df.columns[1:-1],y_vars=['dt_s'], hue='facies',\n",
        "             plot_kws={'alpha':0.02,'edgecolor':'none'},palette=cmap)\n",
        "# Legend markers are too transparent. Fix that\n",
        "g._legend.remove()\n",
        "plt.legend(handles=patchList,bbox_to_anchor=(2, 1),ncol=1)\n",
        "plt.show() # lots of points, takes a few seconds...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VaKlcBASoGl"
      },
      "source": [
        "### Feature selection and Dataset splitting\n",
        "\n",
        "As domain experts, it is our responsibility to choose the best **features** as inputs to the regression models, being mindful not only that the selected features are truly informative for estimating the target, but also that these choices will impact the future applicability of the model, since all input features must be available in future wells for the inference to be possible.\n",
        "\n",
        "We will use the more diverse `well_0` as a **training dataset**, to  **optimize** the **loss function** and obtain the optimum **model parameters** for several models.\n",
        "\n",
        "The **validation dataset** `well_1` will be used to **select the best model** for future use.\n",
        "\n",
        "\n",
        "## **Discussion question**\n",
        "\n",
        "Would a **random selection** of examples from **both wells** be an appropriate train/validation splitting strategy? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:54:55.991002Z",
          "start_time": "2023-01-23T16:54:55.968858Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:30:30.930201Z",
          "iopub.status.busy": "2025-09-19T20:30:30.929850Z",
          "iopub.status.idle": "2025-09-19T20:30:30.940635Z",
          "shell.execute_reply": "2025-09-19T20:30:30.940007Z",
          "shell.execute_reply.started": "2025-09-19T20:30:30.930177Z"
        },
        "id": "6vsdSpG1Zc4d"
      },
      "outputs": [],
      "source": [
        "features = [ ... , ... , ... ] # YOUR CODE HERE. Should we use 'z'? 'What about 'nphi'?\n",
        "target = 'dt_s'\n",
        "\n",
        "mask_train = df['well']=='well_0'  # logical maks\n",
        "mask_validation = ~mask_train\n",
        "\n",
        "df_train = df[mask_train]\n",
        "df_validation = df[mask_validation]\n",
        "\n",
        "X_train = df_train[features]\n",
        "y_train = df_train[target]\n",
        "\n",
        "X_validation = df_validation[features]\n",
        "y_validation = df_validation[target]\n",
        "\n",
        "display(X_train.sample(3), y_train.sample(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFWad-4SoGg"
      },
      "source": [
        "# Multilinear Regression\n",
        "\n",
        "One of the simplest and most flexible models for predicting continuous outcomes is the multilinear regression model:\n",
        "\n",
        "$$y_i \\approx \\hat{y}_i = w_0 + w_1 x_{i1} + w_2 x_{i2} + \\cdots + w_M x_{iM} = \\vec{x}_i\\cdot \\vec{w}\n",
        "$$\n",
        "\n",
        "Where $y_i$ is the **ground truth** for the **target** variable of the $i$-th example, which we would like to predict based on its $M$ known **features** $x_{ij}$ ($j=1,2,\\dots,M$). The **prediction** $\\hat{y}_i$ is simply a linear combination of the **features**.\n",
        "\n",
        "\n",
        "Each example is represented by its (augmented) **feature vector** $\\vec{x}_i$, where we define $x_{i0}=1$ for all examples, so the **inference** step can be written as a single dot product between the **feature vector** ($\\vec{x}_i$) and the **weight vector** ($\\vec{w}$).\n",
        "\n",
        "$$\n",
        "\\vec{x}_i = \\begin{bmatrix} 1 \\\\ x_{i1} \\\\ \\vdots \\\\ x_{iM}\n",
        "\\end{bmatrix} \\qquad ; \\qquad\n",
        "\\vec{w} = \\begin{bmatrix} w_0\n",
        " \\\\ w_1 \\\\ \\vdots \\\\ w_M\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The model **parameters** $w_j$ are known as its **weights** ($w_j$, for $j\\geq1$) and **bias** ($w_0$).\n",
        "\n",
        "It is common to organize the entire dataset of $N$ examples in a matrix $X$ of dimensions $N\\times M+1$ like so:\n",
        "\n",
        "$$X = \\begin{bmatrix} \\vec{x}_1^T \\\\ \\vec{x}_2^T \\\\ \\vdots \\\\ \\vec{x}_N^T\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "1 & x_{11} & x_{12} & \\cdots & x_{1M} \\\\\n",
        "1 & x_{21} & x_{22} & \\cdots & x_{2M} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{N1} & x_{N2} & \\cdots & x_{NM} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "where each line represents a different example, and each column is a different feature, including the constant $1$ feature. This way, the multilinear model can be written as a single matrix equation:\n",
        "\n",
        "$$\n",
        "X\\vec{w} = \\vec{\\hat{y}} \\qquad ; \\qquad \\vec{\\hat{y}} \\approx \\vec{y}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAo7sb20Zc4f"
      },
      "source": [
        "It is important to choose an **objective function** (also called a **loss function**) that expresses mathematically how good the model is. The most common objetive function is the **Sum of Squared Errors**:\n",
        "\n",
        "$$\n",
        "SSE = \\sum_{i=1}^N \\left( \\hat{y}_i - y_i \\right)^2 = \\lVert \\vec{\\hat y} - \\vec{y} \\rVert^2 = \\lVert X\\vec{w} - \\vec{y} \\rVert^2\n",
        "$$\n",
        "\n",
        "The problem of obtaining the best parameters can then be written as:\n",
        "\n",
        "$$\n",
        "\\underset{\\vec{w}}{\\text{argmin}} \\quad  \\lVert X\\vec{w} - \\vec{y} \\rVert^2\n",
        "$$\n",
        "\n",
        "This particular problem has a closed form analytical solution given by solving the so-called normal equations:\n",
        "\n",
        "$$\\begin{align}\n",
        "X^TX\\vec{w} = &X^T\\vec{y}\\\\\n",
        "\\vec{w} = & \\left(X^TX\\right)^{-1}X^T\\vec{y}\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J8U5aJDSoGm"
      },
      "source": [
        "### Degree 1 model - Numpy\n",
        "\n",
        "There are several methods for solving the normal equations. One of the most widely used packages for these kinds of linear algebra problems is `numpy.linalg`, which implements the function `np.linalg.lstsq`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:56:26.916024Z",
          "start_time": "2023-01-23T16:56:25.706346Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:30:31.674037Z",
          "iopub.status.busy": "2025-09-19T20:30:31.673705Z",
          "iopub.status.idle": "2025-09-19T20:30:34.262922Z",
          "shell.execute_reply": "2025-09-19T20:30:34.261839Z",
          "shell.execute_reply.started": "2025-09-19T20:30:31.674015Z"
        },
        "id": "6N5fMRnJSoGm"
      },
      "outputs": [],
      "source": [
        "X1_train = np.column_stack([np.ones(X_train.shape[0]), X_train]) # Adding a column of the constant 1 feature\n",
        "\n",
        "w1,*_ = np.linalg. ... ( ... , ... ) # YOUR CODE HERE. Solving the least squares problem, TRAINING the model\n",
        "print(f'Optimum bias and weights: {w1 = }')\n",
        "\n",
        "y_hat1_train = ... @ ... # YOUR CODE HERE. Using the model, running INFERENCE on the TRAINING SET\n",
        "\n",
        "\n",
        "xlabel = 'Predicted Dt_s'\n",
        "ylabel = 'Measured Dt_s'\n",
        "per_facies_scatter(y_hat1_train, y_train, df_train['facies'],\n",
        "                   xlabel=xlabel, ylabel=ylabel, suptitle='Single degree 1 model (train)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gonfQgbyZc4g"
      },
      "source": [
        "### Validation dataset\n",
        "\n",
        "However, we are interested in how well the model performs when presented with data it has never seen before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-19T20:30:34.265396Z",
          "iopub.status.busy": "2025-09-19T20:30:34.264841Z",
          "iopub.status.idle": "2025-09-19T20:30:37.418129Z",
          "shell.execute_reply": "2025-09-19T20:30:37.416457Z",
          "shell.execute_reply.started": "2025-09-19T20:30:34.265357Z"
        },
        "id": "Ff2v25xiZc4g"
      },
      "outputs": [],
      "source": [
        "X1_validation = np.column_stack([np.ones(X_validation.shape[0]), X_validation]) # Adding a column of the constant 1 feature\n",
        "y_hat1_validation = ... @ ... # YOUR CODE HERE. Using the model, running INFERENCE on the VALIDATION DATASET\n",
        "\n",
        "per_facies_scatter(y_hat1_validation, y_validation, df_validation['facies'],\n",
        "                   xlabel=xlabel, ylabel=ylabel, suptitle='Single degree 1 model (validation)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7iLlyfOZc4h"
      },
      "source": [
        "### Degree 1 model - Scikit-learn\n",
        "\n",
        "For ML tasks, the `sklearn` module implements many of the most widely used models. Linear Regression is available via `skl.LinearRegression`, which implements the methods `.fit` and `.predict`.\n",
        "\n",
        "The model parameters are stored in the attributes `.intercept_` and `.coef_`. The presence of these attributes in sklearn estimators indicates if the estimator has already been trained. By convention, variables which end with an underscore (`_`) are supposed to only be used internally, so be careful with those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:56:26.916024Z",
          "start_time": "2023-01-23T16:56:25.706346Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:30:44.196515Z",
          "iopub.status.busy": "2025-09-19T20:30:44.196184Z",
          "iopub.status.idle": "2025-09-19T20:30:44.213805Z",
          "shell.execute_reply": "2025-09-19T20:30:44.212715Z",
          "shell.execute_reply.started": "2025-09-19T20:30:44.196491Z"
        },
        "id": "HKb6NAbbZc4i"
      },
      "outputs": [],
      "source": [
        "model1 = skl.LinearRegression(fit_intercept=True) # instantiating the model\n",
        "model1. ... (X_train, y_train) # TRAINING the model. No need to add the 1 column because fit_intercept=True handles that for us\n",
        "\n",
        "# Verifying the parameters are the same ones we found earlier\n",
        "print(f'''numpy:\n",
        " {w1 = }\n",
        "\n",
        "sklearn:\n",
        " {model1.intercept_ = }\n",
        " {model1.coef_ = }''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:56:26.916024Z",
          "start_time": "2023-01-23T16:56:25.706346Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:30:50.097124Z",
          "iopub.status.busy": "2025-09-19T20:30:50.096802Z",
          "iopub.status.idle": "2025-09-19T20:30:55.516566Z",
          "shell.execute_reply": "2025-09-19T20:30:55.515133Z",
          "shell.execute_reply.started": "2025-09-19T20:30:50.097102Z"
        },
        "id": "qFDdfOgtZc4j"
      },
      "outputs": [],
      "source": [
        "y_hat1_train = model1.predict(X_train) # Running INFERENCE on the TRAINING SET\n",
        "y_hat1_validation = model1. ... ( ... ) # YOUR CODE HERE to run inference on the VALIDATION SET\n",
        "\n",
        "per_facies_scatter(y_hat1_train, y_train, df_train['facies'],\n",
        "                   xlabel=xlabel, ylabel=ylabel, suptitle='Single degree 1 model (train)');\n",
        "per_facies_scatter(y_hat1_validation, y_validation, df_validation['facies'],\n",
        "                   xlabel=xlabel, ylabel=ylabel, suptitle='Single degree 1 model (validation)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpus9J24SoGm"
      },
      "source": [
        "## Underfitting and Feature engineering\n",
        "\n",
        "It seems the model consistently underestimates the target variable on some regions of the data range, while overestimating it around others, even for data it was trained on. This behaviour is called **underfitting**, and is typical of models that are insuficiently flexible, unable to capture more complex dependencies between the input features and the target variable.\n",
        "\n",
        "The target variable **dt_s** might have a nonlinear dependence on the input variables. One way to account for that is by creating new features from nonlinear combination of the original features. These new features can be handcrafted by using specialist domain knowledge, or simply generated from a stock of premade recipes.\n",
        "\n",
        "A common way of achieving this is by creating **polynomial features** of up to degree $N$ (a **hyperparameter**, which is chosen by the scientist, not optimized by the loss function). This functionality is available via `skp.PolynomialFeatures`, which implements the `.fit` and `.transform` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:56:28.484753Z",
          "start_time": "2023-01-23T16:56:27.305362Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:31:07.752980Z",
          "iopub.status.busy": "2025-09-19T20:31:07.752649Z",
          "iopub.status.idle": "2025-09-19T20:31:07.776948Z",
          "shell.execute_reply": "2025-09-19T20:31:07.775762Z",
          "shell.execute_reply.started": "2025-09-19T20:31:07.752958Z"
        },
        "id": "a_XEx6qoSoGm"
      },
      "outputs": [],
      "source": [
        "poly = skp.PolynomialFeatures(degree= ... ) # change the maximum degree to see what happens\n",
        "poly. ... (X_train)  # there are NO learned parameters here. The fit method only binds the names of the features to apply the transformation\n",
        "X_poly = poly. ... (X_train) # .transform applies the transformation\n",
        "\n",
        "pd.DataFrame(X_poly,columns=poly.get_feature_names_out()).sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNwM1VI2Zc4l"
      },
      "source": [
        "### Pipeline\n",
        "\n",
        "Since all data that goes into our model will have to go through the **same preprocessing and feature engineering** steps before being passed throught to the linear regressor, it is convenient to store all these steps into a single `skpp.Pipeline` object, which automates this chain of compositions. These can easily be created with the `skpp.make_pipeline` convenience function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:56:28.484753Z",
          "start_time": "2023-01-23T16:56:27.305362Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:44:07.267933Z",
          "iopub.status.busy": "2025-09-19T20:44:07.267548Z",
          "iopub.status.idle": "2025-09-19T20:44:07.277958Z",
          "shell.execute_reply": "2025-09-19T20:44:07.277015Z",
          "shell.execute_reply.started": "2025-09-19T20:44:07.267908Z"
        },
        "id": "q6Ckahk6Zc4l"
      },
      "outputs": [],
      "source": [
        "pipe2 = skpp.make_pipeline(\n",
        "    skp.PolynomialFeatures(degree=2, include_bias=False), # no need to create the constant column, since the LinearRegression handles that\n",
        "    skl.LinearRegression() )\n",
        "pipe2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihXbrmU-Zc4l"
      },
      "source": [
        "`skpp.Pipeline` objects also have the `.fit`, `.transform` and `.predict` methods, which are run sequentially on each of its steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:56:28.484753Z",
          "start_time": "2023-01-23T16:56:27.305362Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:44:13.145984Z",
          "iopub.status.busy": "2025-09-19T20:44:13.145644Z",
          "iopub.status.idle": "2025-09-19T20:44:13.166585Z",
          "shell.execute_reply": "2025-09-19T20:44:13.165987Z",
          "shell.execute_reply.started": "2025-09-19T20:44:13.145960Z"
        },
        "id": "1OUXBqsTZc4m"
      },
      "outputs": [],
      "source": [
        "pipe2.fit(X_train, y_train)  # YOUR CODE HERE. Training the model on the TRAINING SET\n",
        "pipe2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:56:28.484753Z",
          "start_time": "2023-01-23T16:56:27.305362Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:44:18.454638Z",
          "iopub.status.busy": "2025-09-19T20:44:18.454317Z",
          "iopub.status.idle": "2025-09-19T20:44:23.533790Z",
          "shell.execute_reply": "2025-09-19T20:44:23.530909Z",
          "shell.execute_reply.started": "2025-09-19T20:44:18.454616Z"
        },
        "id": "YxvotYQKZc4m"
      },
      "outputs": [],
      "source": [
        "y_hat2_train = pipe2. ...  # YOUR CODE HERE to apply the pipeline to the TRAINING SET\n",
        "y_hat2_validation = pipe2. ...  # YOUR CODE HERE to apply the pipeline to the VALIDATION SET\n",
        "\n",
        "deg = pipe2['polynomialfeatures'].degree # each step of the pipeline can be accessed via its name, if needed\n",
        "\n",
        "per_facies_scatter(y_hat2_train, y_train, df_train['facies'],\n",
        "                   xlabel=xlabel, ylabel=ylabel, suptitle=f'Single degree {deg} model (train)');\n",
        "per_facies_scatter(y_hat2_validation, y_validation, df_validation['facies'],\n",
        "                   xlabel=xlabel, ylabel=ylabel, suptitle=f'Single degree {deg} model (validation)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VEjKAmgSoGn"
      },
      "source": [
        "## Specialized models for each facies\n",
        "\n",
        "The model's performance varies across each facies, performing better on some classes than on others. Since some facies are underrepresented in the dataset, the single model we created is not incentivized to accurately predict **dt_s** for these rare facies. It is often worth it to create specialized models for each facies (or group of facies)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:56:34.477133Z",
          "start_time": "2023-01-23T16:56:33.538143Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:44:24.509011Z",
          "iopub.status.busy": "2025-09-19T20:44:24.508692Z",
          "iopub.status.idle": "2025-09-19T20:44:30.015194Z",
          "shell.execute_reply": "2025-09-19T20:44:30.013364Z",
          "shell.execute_reply.started": "2025-09-19T20:44:24.508988Z"
        },
        "id": "YJaCqVnZSoGn"
      },
      "outputs": [],
      "source": [
        "pipes3 = dict() # initializing an empty dict\n",
        "\n",
        "# Training data set\n",
        "y_hat3_train = 0*y_train # initializing a zeroed vector of predictions\n",
        "for fac in facies_list:\n",
        "    fac_pipe = skpp.make_pipeline(skp. ... (degree=2,include_bias=False),\n",
        "                                  skl. ...\n",
        "                                  )\n",
        "    mask_fac_train = df_train['facies'] == fac # logical mask\n",
        "\n",
        "    fac_pipe. ... (X_train[mask_fac_train],y_train[mask_fac_train]) # training only on the relevant facies\n",
        "\n",
        "    pipes3[fac] = fac_pipe # storing the pipeline for later use\n",
        "\n",
        "    y_hat3_train[mask_fac_train] = pipes3[fac].predict(X_train[mask_fac_train])\n",
        "\n",
        "# Validation data set\n",
        "y_hat3_validation = 0*y_validation # initializing a zeroed vector of predictions\n",
        "for fac in facies_list:\n",
        "    mask_fac_validation = df_validation['facies'] == fac # logical mask\n",
        "    if not mask_fac_validation.any():\n",
        "       continue # if there are no data points of the given facies, no need to run the model\n",
        "    y_hat3_validation[mask_fac_validation] = pipes3[fac]. ... (X_validation[mask_fac_validation]) # YOUR CODE HERE\n",
        "\n",
        "per_facies_scatter(y_hat3_train, y_train, df_train['facies'],\n",
        "                   xlabel=xlabel, ylabel=ylabel,\n",
        "                   suptitle=f'Facies Specialized degree {fac_pipe[\"polynomialfeatures\"].degree} model (train)');\n",
        "per_facies_scatter(y_hat3_validation, y_validation, df_validation[\"facies\"],\n",
        "                   xlabel=xlabel, ylabel=ylabel,\n",
        "                   suptitle=f'Facies Specialized degree {fac_pipe[\"polynomialfeatures\"].degree} model (validation)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpWW-ogHZc4t"
      },
      "source": [
        "### Overfitting\n",
        "\n",
        "It might be reasonable to assume that a more specialized model, or one with more adjustable parameters, will be able to fit the data more closely. While that is true of the **training set**, it might not be so for novel data (represented here by the **validation set**). If it is the case that the model performance is good on the training data, but is significantly worse for novel data, we say that the model does **not generalize** well, it **overfits** the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-19T20:44:32.591053Z",
          "iopub.status.busy": "2025-09-19T20:44:32.590269Z",
          "iopub.status.idle": "2025-09-19T20:44:37.889436Z",
          "shell.execute_reply": "2025-09-19T20:44:37.888126Z",
          "shell.execute_reply.started": "2025-09-19T20:44:32.591024Z"
        },
        "id": "c_SJytSZZc4t"
      },
      "outputs": [],
      "source": [
        "pipes4 = dict() # initializing an empty dict of models\n",
        "\n",
        "# Training data set\n",
        "y_hat4_train = 0*y_train # initializing a zeroed vector of predictions\n",
        "for fac in facies_list:\n",
        "    fac_pipe = skpp.make_pipeline(skp.PolynomialFeatures(degree= ... ,include_bias=False), # Try several degrees\n",
        "                                  skl.LinearRegression()\n",
        "                                     )\n",
        "    mask_fac_train = df_train['facies'] == fac # logical mask\n",
        "    fac_pipe.fit(X_train[mask_fac_train],y_train[mask_fac_train]) # training only on the relevant facies\n",
        "    pipes4[fac] = fac_pipe # storing the pipeline for later use\n",
        "\n",
        "    y_hat4_train[mask_fac_train] = pipes4[fac].predict(X_train[mask_fac_train])\n",
        "\n",
        "# Validation data set\n",
        "y_hat4_validation = 0*y_validation # initializing a zeroed vector of predictions\n",
        "for fac in facies_list:\n",
        "    mask_fac_validation = df_validation['facies'] == fac # logical mask\n",
        "    if not mask_fac_validation.any():\n",
        "       continue # if there are no data points of the given facies, no need to run the model\n",
        "    y_hat4_validation[mask_fac_validation] = pipes4[fac].predict(X_validation[mask_fac_validation]) # YOUR CODE HERE\n",
        "\n",
        "per_facies_scatter(y_hat4_train, y_train, df_train['facies'],\n",
        "                   xlabel=xlabel, ylabel=ylabel,\n",
        "                   suptitle=f'Facies Specialized degree {fac_pipe[\"polynomialfeatures\"].degree} model (train)');\n",
        "per_facies_scatter(y_hat4_validation, y_validation, df_validation[\"facies\"],\n",
        "                   xlabel=xlabel, ylabel=ylabel,\n",
        "                   suptitle=f'Facies Specialized degree {fac_pipe[\"polynomialfeatures\"].degree} model (validation)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB8F6vC3SoGo"
      },
      "source": [
        "There are many metrics we can use to judge model performance. Many of them are implemented in `sklearn.metrics` (and there are many more which are not).\n",
        "\n",
        "The best performing model (along with all preprocessing steps) as measured by the performance on the **validation dataset** should be stored for later use.\n",
        "\n",
        "The best practice is to also do a unbiased estimation of its performance on a proxy of future data, that is, neither the training set (which is biased since it was used for parameter optimization), nor the validation set (which is biased since it was used for hyperparameter tuning). This extra **holdout data** is usually called the **test set**, though some sources reverse the use of \"validation\" and \"test\", and some others use them interchangeably.\n",
        "\n",
        "$$\\begin{align}\n",
        "MSE & = \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i - y_i \\right)^2\\\\\n",
        "RMSE & = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{y}_i - y_i \\right)^2} \\\\\n",
        "MAE & = \\frac{1}{N}\\sum_{i=1}^N \\lvert\\hat{y}_i - y_i \\rvert \\\\\n",
        "R^2 & = 1 - \\frac{\\displaystyle\\sum_{i=1}^N\\left(\\hat{y}_i - y_i \\right)^2}{\\displaystyle\\sum_{i=1}^N\\left(\\bar{y} - y_i \\right)^2} \\quad ; \\quad \\bar{y} = \\frac{1}{N}\\sum_{i=1}^Ny_i\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-23T16:57:31.602004Z",
          "start_time": "2023-01-23T16:57:31.593251Z"
        },
        "execution": {
          "iopub.execute_input": "2025-09-19T20:44:46.270358Z",
          "iopub.status.busy": "2025-09-19T20:44:46.269820Z",
          "iopub.status.idle": "2025-09-19T20:44:46.284372Z",
          "shell.execute_reply": "2025-09-19T20:44:46.283500Z",
          "shell.execute_reply.started": "2025-09-19T20:44:46.270329Z"
        },
        "id": "eQgNc4OtSoGo"
      },
      "outputs": [],
      "source": [
        "#                                                 sklearn                                                 numpy\n",
        "print('Mean Squared Error (train)      :',skm.mean_squared_error(y_train,y_hat4_train), ' ; ', np.mean((y_train-y_hat4_train)**2))\n",
        "print('Mean Squared Error (validation) :',skm.mean_squared_error(y_validation,y_hat4_validation ))\n",
        "print()\n",
        "print('Root Mean Squared Error (train)      :',np.sqrt(skm.mean_squared_error(y_train,y_hat4_train)))\n",
        "print('Root Mean Squared Error (validation) :',np.sqrt(skm.mean_squared_error(y_validation,y_hat4_validation )))\n",
        "print() #                                        sklearn                                                   numpy\n",
        "print('Mean Absolute Error (train)      :',skm.mean_absolute_error(y_train,y_hat4_train),' ; ', np.mean(np.abs(y_train-y_hat4_train)))\n",
        "print('Mean Absolute Error (validation) :',skm.mean_absolute_error(y_validation,y_hat4_validation ))\n",
        "print() #                                        sklearn                                          numpy\n",
        "print('R2 (train)      :',skm.r2_score(y_train,y_hat4_train), ' ; ', 1 - np.mean((y_train-y_hat4_train)**2)/np.mean((y_train-y_train.mean())**2))\n",
        "print('R2 (validation) :',skm.r2_score(y_validation,y_hat4_validation ))\n",
        "print()\n",
        "print('Max Absolute Error (train)      :', np.max(np.abs(y_train-y_hat4_train)))\n",
        "print('Max Absolute Error (validation) :', np.max(np.abs(y_validation-y_hat4_validation)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd5yzzaUSoGo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GArhbaCSoGo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NHIJd3SSoGo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWcjFflYSoGo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gTOY2TTSoGo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}